{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-20T04:12:12.225892Z","iopub.status.busy":"2024-10-20T04:12:12.22505Z","iopub.status.idle":"2024-10-20T04:12:34.88679Z","shell.execute_reply":"2024-10-20T04:12:34.885644Z","shell.execute_reply.started":"2024-10-20T04:12:12.225833Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import json\n","from sklearn.base import clone\n","from sklearn.metrics import cohen_kappa_score, make_scorer\n","from sklearn.model_selection import StratifiedKFold, cross_val_score\n","from scipy.optimize import minimize\n","from concurrent.futures import ThreadPoolExecutor\n","from tqdm import tqdm\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import keras\n","from keras.models import Model\n","from keras.layers import Input, Dense\n","from keras.optimizers import Adam\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from colorama import Fore, Style\n","from IPython.display import clear_output\n","import warnings\n","from lightgbm import LGBMRegressor, LGBMClassifier\n","from xgboost import XGBRegressor, XGBClassifier\n","from catboost import CatBoostRegressor, CatBoostClassifier\n","from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor, VotingClassifier\n","from sklearn.impute import SimpleImputer, KNNImputer\n","from sklearn.pipeline import Pipeline\n","from scipy import stats\n","import optuna"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Create output folders\n","output_folder = 'output'\n","if not os.path.exists(output_folder):\n","    os.makedirs(output_folder)\n","\n","# Create separate analysis output folders\n","analysis_output_folder = 'analysis_output'\n","os.makedirs(analysis_output_folder, exist_ok=True)\n","\n","physical_analysis_output_folder = 'analysis_output/physical'\n","os.makedirs(physical_analysis_output_folder, exist_ok=True)\n","\n","fitness_analysis_output_folder = 'analysis_output/fitness'\n","os.makedirs(fitness_analysis_output_folder, exist_ok=True)\n","\n","bia_analysis_output_folder = 'analysis_output/bia'\n","os.makedirs(bia_analysis_output_folder, exist_ok=True)\n","\n","child_info_analysis_output_folder = 'analysis_output/child_info'\n","os.makedirs(child_info_analysis_output_folder, exist_ok=True)\n","\n","actigraphy_analysis_output_folder = 'analysis_output/actigraphy'\n","os.makedirs(actigraphy_analysis_output_folder, exist_ok=True)\n","\n","# Set display all columns in dataframes property\n","pd.options.display.max_columns = None\n","\n","# Supress warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-20T04:12:34.889624Z","iopub.status.busy":"2024-10-20T04:12:34.888832Z","iopub.status.idle":"2024-10-20T04:12:34.89971Z","shell.execute_reply":"2024-10-20T04:12:34.898226Z","shell.execute_reply.started":"2024-10-20T04:12:34.88958Z"},"trusted":true},"outputs":[],"source":["# Load and process data files\n","def process_file(filename, dirname):\n","    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n","    df.drop('step', axis=1, inplace=True)\n","    return df.describe().values.reshape(-1), filename.split('=')[1]\n","\n","# Load time series data\n","def load_time_series(dirname) -> pd.DataFrame:\n","    ids = os.listdir(dirname)\n","    \n","    with ThreadPoolExecutor() as executor:\n","        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n","    \n","    stats, indexes = zip(*results)\n","    \n","    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n","    df['id'] = indexes\n","    return df"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 996/996 [01:03<00:00, 15.67it/s]\n","100%|██████████| 2/2 [00:00<00:00,  7.62it/s]\n"]}],"source":["# Load data\n","train = pd.read_csv('input/train.csv')\n","test = pd.read_csv('input/test.csv')\n","sample = pd.read_csv('input/sample_submission.csv')\n","\n","train_ts = load_time_series(\"input/series_train.parquet\")\n","test_ts = load_time_series(\"input/series_test.parquet\")\n","\n","df_train = train_ts.drop('id', axis=1)\n","df_test = test_ts.drop('id', axis=1)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-20T04:12:34.902343Z","iopub.status.busy":"2024-10-20T04:12:34.901837Z","iopub.status.idle":"2024-10-20T04:12:34.934431Z","shell.execute_reply":"2024-10-20T04:12:34.933012Z","shell.execute_reply.started":"2024-10-20T04:12:34.902256Z"},"trusted":true},"outputs":[],"source":["# Sparse Autoencoder Model\n","class SparseAutoencoder(nn.Module):\n","    def __init__(self, input_dim, sparsity_weight=1e-5):\n","        super(SparseAutoencoder, self).__init__()\n","        self.sparsity_weight = sparsity_weight\n","        self.encoder = nn.Sequential(\n","            nn.Linear(input_dim, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 32),\n","            nn.ReLU(),\n","            nn.Linear(32, 16),\n","            nn.ReLU()\n","        )\n","        \n","        self.decoder = nn.Sequential(\n","            nn.Linear(16, 32),\n","            nn.ReLU(),\n","            nn.Linear(32, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, input_dim),\n","            nn.Sigmoid()  # Outputs in the range [0, 1]\n","        )\n","        \n","    def forward(self, x):\n","        encoded = self.encoder(x)\n","        decoded = self.decoder(encoded)\n","        return encoded, decoded\n","\n","# Preparing Data\n","# Option to use different scalers: MinMaxScaler, StandardScaler, RobustScaler\n","def prepare_data(data, scaler_type='MinMaxScaler'):\n","    if scaler_type == 'StandardScaler':\n","        scaler = StandardScaler()\n","    elif scaler_type == 'RobustScaler':\n","        scaler = RobustScaler()\n","    else:\n","        scaler = MinMaxScaler()\n","    \n","    data_scaled = scaler.fit_transform(data)\n","    return torch.tensor(data_scaled, dtype=torch.float32), scaler\n","\n","# Apply PCA for Dimensionality Reduction\n","# This can help focus the autoencoder on the most relevant features\n","def apply_pca(data, n_components=0.95):\n","    pca = PCA(n_components=n_components)\n","    data_pca = pca.fit_transform(data)\n","    return data_pca, pca\n","\n","# Early Stopping Functionality\n","def early_stopping(patience):\n","    class EarlyStopping:\n","        def __init__(self, patience=patience):\n","            self.patience = patience\n","            self.counter = 0\n","            self.best_loss = float('inf')\n","            self.early_stop = False\n","        \n","        def __call__(self, loss):\n","            if loss < self.best_loss:\n","                self.best_loss = loss\n","                self.counter = 0\n","            else:\n","                self.counter += 1\n","                if self.counter >= self.patience:\n","                    self.early_stop = True\n","    return EarlyStopping()\n","\n","# Training the Sparse Autoencoder with DataFrame Output\n","def perform_autoencoder(data, epochs=100, batch_size=32, learning_rate=0.001, patience=10, scaler_type='MinMaxScaler', use_pca=False, sparsity_weight=1e-5):\n","    # Preprocess Data\n","    if use_pca:\n","        data, pca = apply_pca(data)\n","\n","    data_tensor, scaler = prepare_data(data, scaler_type=scaler_type)\n","    train_data, val_data = train_test_split(data_tensor, test_size=0.2, random_state=42)\n","\n","    train_loader = DataLoader(TensorDataset(train_data), batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(TensorDataset(val_data), batch_size=batch_size, shuffle=False)\n","\n","    model = SparseAutoencoder(input_dim=data.shape[1], sparsity_weight=sparsity_weight)\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","\n","    criterion = nn.SmoothL1Loss()  # Changed to Smooth L1 Loss\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    stopper = early_stopping(patience=patience)\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        train_loss = 0.0\n","        for batch in train_loader:\n","            batch = batch[0].to(device)\n","            optimizer.zero_grad()\n","            encoded, outputs = model(batch)\n","            \n","            # Reconstruction loss\n","            loss = criterion(outputs, batch)\n","            \n","            # Sparsity penalty (L1 regularization on encoded activations)\n","            l1_penalty = torch.mean(torch.abs(encoded))\n","            loss += sparsity_weight * l1_penalty\n","            \n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item() * batch.size(0)\n","\n","        train_loss /= len(train_loader.dataset)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                batch = batch[0].to(device)\n","                _, outputs = model(batch)\n","                loss = criterion(outputs, batch)\n","                val_loss += loss.item() * batch.size(0)\n","\n","        val_loss /= len(val_loader.dataset)\n","        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n","\n","        # Early stopping\n","        stopper(val_loss)\n","        if stopper.early_stop:\n","            print(f\"Early stopping at epoch {epoch + 1}\")\n","            break\n","\n","    # Convert tensor back to DataFrame for consistency\n","    _, data_decoded = model(data_tensor.to(device))\n","    data_decoded = data_decoded.cpu().detach().numpy()\n","    df_encoded = pd.DataFrame(data_decoded, columns=[f'feature_{i}' for i in range(data_decoded.shape[1])])\n","    return df_encoded\n","\n","# Usage example\n","# Assuming 'data' is your input dataset as a NumPy array or pandas DataFrame.\n","# df_encoded = train_sparse_autoencoder(data, epochs=100, batch_size=32, learning_rate=0.001, patience=10, scaler_type='StandardScaler', use_pca=True, sparsity_weight=1e-5)\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Train Loss: 0.0480, Validation Loss: 0.0419\n","Epoch 2, Train Loss: 0.0299, Validation Loss: 0.0214\n","Epoch 3, Train Loss: 0.0192, Validation Loss: 0.0184\n","Epoch 4, Train Loss: 0.0182, Validation Loss: 0.0182\n","Epoch 5, Train Loss: 0.0180, Validation Loss: 0.0179\n","Epoch 6, Train Loss: 0.0179, Validation Loss: 0.0178\n","Epoch 7, Train Loss: 0.0176, Validation Loss: 0.0173\n","Epoch 8, Train Loss: 0.0166, Validation Loss: 0.0157\n","Epoch 9, Train Loss: 0.0141, Validation Loss: 0.0129\n","Epoch 10, Train Loss: 0.0122, Validation Loss: 0.0116\n","Epoch 11, Train Loss: 0.0113, Validation Loss: 0.0108\n","Epoch 12, Train Loss: 0.0108, Validation Loss: 0.0105\n","Epoch 13, Train Loss: 0.0105, Validation Loss: 0.0101\n","Epoch 14, Train Loss: 0.0103, Validation Loss: 0.0100\n","Epoch 15, Train Loss: 0.0100, Validation Loss: 0.0097\n","Epoch 16, Train Loss: 0.0097, Validation Loss: 0.0090\n","Epoch 17, Train Loss: 0.0087, Validation Loss: 0.0077\n","Epoch 18, Train Loss: 0.0072, Validation Loss: 0.0061\n","Epoch 19, Train Loss: 0.0062, Validation Loss: 0.0057\n","Epoch 20, Train Loss: 0.0060, Validation Loss: 0.0056\n","Epoch 21, Train Loss: 0.0060, Validation Loss: 0.0055\n","Epoch 22, Train Loss: 0.0059, Validation Loss: 0.0054\n","Epoch 23, Train Loss: 0.0059, Validation Loss: 0.0054\n","Epoch 24, Train Loss: 0.0058, Validation Loss: 0.0053\n","Epoch 25, Train Loss: 0.0058, Validation Loss: 0.0054\n","Epoch 26, Train Loss: 0.0057, Validation Loss: 0.0053\n","Epoch 27, Train Loss: 0.0057, Validation Loss: 0.0053\n","Epoch 28, Train Loss: 0.0057, Validation Loss: 0.0053\n","Epoch 29, Train Loss: 0.0057, Validation Loss: 0.0053\n","Epoch 30, Train Loss: 0.0056, Validation Loss: 0.0052\n","Epoch 31, Train Loss: 0.0056, Validation Loss: 0.0052\n","Epoch 32, Train Loss: 0.0056, Validation Loss: 0.0052\n","Epoch 33, Train Loss: 0.0056, Validation Loss: 0.0052\n","Epoch 34, Train Loss: 0.0056, Validation Loss: 0.0052\n","Epoch 35, Train Loss: 0.0056, Validation Loss: 0.0051\n","Epoch 36, Train Loss: 0.0055, Validation Loss: 0.0052\n","Epoch 37, Train Loss: 0.0055, Validation Loss: 0.0052\n","Epoch 38, Train Loss: 0.0055, Validation Loss: 0.0052\n","Epoch 39, Train Loss: 0.0055, Validation Loss: 0.0051\n","Epoch 40, Train Loss: 0.0055, Validation Loss: 0.0053\n","Epoch 41, Train Loss: 0.0055, Validation Loss: 0.0051\n","Epoch 42, Train Loss: 0.0054, Validation Loss: 0.0051\n","Epoch 43, Train Loss: 0.0054, Validation Loss: 0.0051\n","Epoch 44, Train Loss: 0.0054, Validation Loss: 0.0050\n","Epoch 45, Train Loss: 0.0053, Validation Loss: 0.0051\n","Epoch 46, Train Loss: 0.0053, Validation Loss: 0.0050\n","Epoch 47, Train Loss: 0.0053, Validation Loss: 0.0051\n","Epoch 48, Train Loss: 0.0053, Validation Loss: 0.0050\n","Epoch 49, Train Loss: 0.0053, Validation Loss: 0.0050\n","Epoch 50, Train Loss: 0.0053, Validation Loss: 0.0050\n","Epoch 51, Train Loss: 0.0053, Validation Loss: 0.0049\n","Epoch 52, Train Loss: 0.0053, Validation Loss: 0.0051\n","Epoch 53, Train Loss: 0.0053, Validation Loss: 0.0050\n","Epoch 54, Train Loss: 0.0052, Validation Loss: 0.0049\n","Epoch 55, Train Loss: 0.0052, Validation Loss: 0.0050\n","Epoch 56, Train Loss: 0.0052, Validation Loss: 0.0049\n","Epoch 57, Train Loss: 0.0052, Validation Loss: 0.0050\n","Epoch 58, Train Loss: 0.0052, Validation Loss: 0.0050\n","Epoch 59, Train Loss: 0.0052, Validation Loss: 0.0049\n","Epoch 60, Train Loss: 0.0051, Validation Loss: 0.0049\n","Epoch 61, Train Loss: 0.0051, Validation Loss: 0.0049\n","Epoch 62, Train Loss: 0.0051, Validation Loss: 0.0049\n","Epoch 63, Train Loss: 0.0051, Validation Loss: 0.0049\n","Epoch 64, Train Loss: 0.0051, Validation Loss: 0.0049\n","Epoch 65, Train Loss: 0.0051, Validation Loss: 0.0048\n","Epoch 66, Train Loss: 0.0051, Validation Loss: 0.0048\n","Epoch 67, Train Loss: 0.0051, Validation Loss: 0.0049\n","Epoch 68, Train Loss: 0.0051, Validation Loss: 0.0048\n","Epoch 69, Train Loss: 0.0051, Validation Loss: 0.0049\n","Epoch 70, Train Loss: 0.0050, Validation Loss: 0.0048\n","Epoch 71, Train Loss: 0.0050, Validation Loss: 0.0048\n","Epoch 72, Train Loss: 0.0050, Validation Loss: 0.0048\n","Epoch 73, Train Loss: 0.0050, Validation Loss: 0.0048\n","Epoch 74, Train Loss: 0.0049, Validation Loss: 0.0047\n","Epoch 75, Train Loss: 0.0049, Validation Loss: 0.0047\n","Epoch 76, Train Loss: 0.0049, Validation Loss: 0.0047\n","Epoch 77, Train Loss: 0.0049, Validation Loss: 0.0047\n","Epoch 78, Train Loss: 0.0049, Validation Loss: 0.0047\n","Epoch 79, Train Loss: 0.0048, Validation Loss: 0.0046\n","Epoch 80, Train Loss: 0.0048, Validation Loss: 0.0046\n","Epoch 81, Train Loss: 0.0048, Validation Loss: 0.0046\n","Epoch 82, Train Loss: 0.0048, Validation Loss: 0.0046\n","Epoch 83, Train Loss: 0.0047, Validation Loss: 0.0045\n","Epoch 84, Train Loss: 0.0046, Validation Loss: 0.0045\n","Epoch 85, Train Loss: 0.0046, Validation Loss: 0.0044\n","Epoch 86, Train Loss: 0.0046, Validation Loss: 0.0044\n","Epoch 87, Train Loss: 0.0045, Validation Loss: 0.0043\n","Epoch 88, Train Loss: 0.0045, Validation Loss: 0.0042\n","Epoch 89, Train Loss: 0.0044, Validation Loss: 0.0042\n","Epoch 90, Train Loss: 0.0044, Validation Loss: 0.0041\n","Epoch 91, Train Loss: 0.0044, Validation Loss: 0.0041\n","Epoch 92, Train Loss: 0.0043, Validation Loss: 0.0040\n","Epoch 93, Train Loss: 0.0043, Validation Loss: 0.0040\n","Epoch 94, Train Loss: 0.0043, Validation Loss: 0.0040\n","Epoch 95, Train Loss: 0.0042, Validation Loss: 0.0040\n","Epoch 96, Train Loss: 0.0042, Validation Loss: 0.0039\n","Epoch 97, Train Loss: 0.0042, Validation Loss: 0.0039\n","Epoch 98, Train Loss: 0.0042, Validation Loss: 0.0039\n","Epoch 99, Train Loss: 0.0041, Validation Loss: 0.0039\n","Epoch 100, Train Loss: 0.0041, Validation Loss: 0.0038\n","Epoch 1, Train Loss: 0.1259, Validation Loss: 0.1240\n","Epoch 2, Train Loss: 0.1254, Validation Loss: 0.1244\n","Epoch 3, Train Loss: 0.1249, Validation Loss: 0.1247\n","Epoch 4, Train Loss: 0.1243, Validation Loss: 0.1251\n","Epoch 5, Train Loss: 0.1238, Validation Loss: 0.1254\n","Epoch 6, Train Loss: 0.1233, Validation Loss: 0.1258\n","Epoch 7, Train Loss: 0.1228, Validation Loss: 0.1262\n","Epoch 8, Train Loss: 0.1222, Validation Loss: 0.1266\n","Epoch 9, Train Loss: 0.1217, Validation Loss: 0.1270\n","Epoch 10, Train Loss: 0.1211, Validation Loss: 0.1274\n","Epoch 11, Train Loss: 0.1205, Validation Loss: 0.1278\n","Early stopping at epoch 11\n"]}],"source":["# Encode time series data\n","train_ts_encoded = perform_autoencoder(df_train, epochs=100, batch_size=32, learning_rate=0.001, patience=10, use_pca=False, scaler_type='MinMaxScaler', sparsity_weight=1e-5)\n","test_ts_encoded = perform_autoencoder(df_test, epochs=100, batch_size=32, learning_rate=0.001, patience=10, use_pca=False, scaler_type='MinMaxScaler', sparsity_weight=1e-5)\n","\n","train_ts_encoded[\"id\"]=train_ts[\"id\"]\n","test_ts_encoded['id']=test_ts[\"id\"]\n","\n","# Merge data\n","train = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\n","test = pd.merge(test, test_ts_encoded, how=\"left\", on='id')\n","\n","train = train.drop('id', axis=1)\n","test = test.drop('id', axis=1)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Transforming column: BIA-BIA_BMC\n","Optimal lambda for Box-Cox transformation: -0.26544288750244394\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 1991\n","Transforming column: BIA-BIA_BMR\n","Optimal lambda for Box-Cox transformation: -2.024016452566404\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 1991\n","Transforming column: BIA-BIA_DEE\n","Optimal lambda for Box-Cox transformation: -0.9862196352522961\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 1991\n","Transforming column: BIA-BIA_ECW\n","Optimal lambda for Box-Cox transformation: -0.11312798067663181\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 1991\n","Transforming column: BIA-BIA_Fat\n","Optimal lambda for Box-Cox transformation: 27.718481796974547\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 1991\n","Transforming column: BIA-BIA_FFM\n","Optimal lambda for Box-Cox transformation: -0.7810475260307541\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 1991\n","Transforming column: BIA-BIA_FFMI\n","Optimal lambda for Box-Cox transformation: -2.6393130908941718\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 1991\n","Transforming column: BIA-BIA_FMI\n","Optimal lambda for Box-Cox transformation: 6.050301094414511\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 1991\n","Transforming column: BIA-BIA_ICW\n","Optimal lambda for Box-Cox transformation: -1.166773892320031\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 1991\n","Transforming column: BIA-BIA_LDM\n","Optimal lambda for Box-Cox transformation: -0.40982691145277933\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 1991\n","Transforming column: BIA-BIA_LST\n","Optimal lambda for Box-Cox transformation: -0.611399096511007\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 1991\n","Transforming column: BIA-BIA_TBW\n","Optimal lambda for Box-Cox transformation: -0.7050562887826972\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 1991\n","Transforming column: CGAS-CGAS_Score\n","Optimal lambda for Box-Cox transformation: -0.16607217346817477\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 2421\n","Transforming column: feature_23\n","Optimal lambda for Box-Cox transformation: 0.7426458969887805\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 996\n","Transforming column: feature_35\n","Optimal lambda for Box-Cox transformation: 0.29314823324538386\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 996\n","Transforming column: feature_38\n","Optimal lambda for Box-Cox transformation: 37.542577569082326\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 996\n","Transforming column: feature_40\n","Optimal lambda for Box-Cox transformation: -0.23986238970498497\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 996\n","Transforming column: feature_47\n","Optimal lambda for Box-Cox transformation: 0.5977171927659619\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 996\n","Transforming column: feature_54\n","Optimal lambda for Box-Cox transformation: -0.1567699536246139\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 996\n","Transforming column: feature_66\n","Optimal lambda for Box-Cox transformation: -0.028666292177620067\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 996\n","Transforming column: feature_78\n","Optimal lambda for Box-Cox transformation: 0.44646443619110426\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 996\n","Transforming column: feature_80\n","Optimal lambda for Box-Cox transformation: -3.534341929949154\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 996\n","Transforming column: feature_88\n","Optimal lambda for Box-Cox transformation: 72.9730109620859\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 996\n","Transforming column: feature_90\n","Optimal lambda for Box-Cox transformation: 1.5098115916902854\n","Number of rows before transformation: 3960\n","Number of rows after removing NaN values: 996\n","Applying transformation to column: BIA-BIA_BMC with lambda: -0.26544288750244394\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 8\n","Applying transformation to column: BIA-BIA_BMR with lambda: -2.024016452566404\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 8\n","Applying transformation to column: BIA-BIA_DEE with lambda: -0.9862196352522961\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 8\n","Applying transformation to column: BIA-BIA_ECW with lambda: -0.11312798067663181\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 8\n","Applying transformation to column: BIA-BIA_Fat with lambda: 27.718481796974547\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 8\n","Applying transformation to column: BIA-BIA_FFM with lambda: -0.7810475260307541\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 8\n","Applying transformation to column: BIA-BIA_FFMI with lambda: -2.6393130908941718\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 8\n","Applying transformation to column: BIA-BIA_FMI with lambda: 6.050301094414511\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 8\n","Applying transformation to column: BIA-BIA_ICW with lambda: -1.166773892320031\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 8\n","Applying transformation to column: BIA-BIA_LDM with lambda: -0.40982691145277933\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 8\n","Applying transformation to column: BIA-BIA_LST with lambda: -0.611399096511007\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 8\n","Applying transformation to column: BIA-BIA_TBW with lambda: -0.7050562887826972\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 8\n","Applying transformation to column: CGAS-CGAS_Score with lambda: -0.16607217346817477\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 8\n","Applying transformation to column: feature_23 with lambda: 0.7426458969887805\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 2\n","Applying transformation to column: feature_35 with lambda: 0.29314823324538386\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 2\n","Applying transformation to column: feature_38 with lambda: 37.542577569082326\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 2\n","Applying transformation to column: feature_40 with lambda: -0.23986238970498497\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 2\n","Applying transformation to column: feature_47 with lambda: 0.5977171927659619\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 2\n","Applying transformation to column: feature_54 with lambda: -0.1567699536246139\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 2\n","Applying transformation to column: feature_66 with lambda: -0.028666292177620067\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 2\n","Applying transformation to column: feature_78 with lambda: 0.44646443619110426\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 2\n","Applying transformation to column: feature_80 with lambda: -3.534341929949154\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 2\n","Applying transformation to column: feature_88 with lambda: 72.9730109620859\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 2\n","Applying transformation to column: feature_90 with lambda: 1.5098115916902854\n","Number of rows before transformation: 20\n","Number of rows after removing NaN values: 2\n"]}],"source":["# Skew removal for some columns\n","skewed_columns = [\n","    'BIA-BIA_BMC', 'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_Fat',\n","    'BIA-BIA_FFM', 'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', \n","    'BIA-BIA_TBW', 'CGAS-CGAS_Score', 'feature_23', 'feature_35', 'feature_38', 'feature_40', 'feature_47',\n","    'feature_54', 'feature_66', 'feature_78', 'feature_80', 'feature_88', 'feature_90'\n","]\n","lambda_params = {}\n","\n","# Define the box-cox function to remove skew\n","def box_cox_transform(df, column, lambda_param=None):\n","    # Create a copy of the dataframe\n","    df_copy = df.copy()\n","    \n","    # Drop NaN values for the specific column\n","    df_copy = df_copy.dropna(subset=[column])\n","    \n","    # Ensure all values are positive\n","    min_value = df_copy[column].min()\n","    if min_value <= 0:\n","        df_copy[column] = df_copy[column] - min_value + 1  # Add 1 to ensure all values are positive\n","    \n","    # Perform Box-Cox transformation\n","    if lambda_param is None:\n","        df_copy[f'{column}_boxcox'], lambda_param = stats.boxcox(df_copy[column])\n","        print(f\"Transforming column: {column}\")\n","        print(f\"Optimal lambda for Box-Cox transformation: {lambda_param}\")\n","    else:\n","        df_copy[f'{column}_boxcox'] = stats.boxcox(df_copy[column], lmbda=lambda_param)\n","        print(f\"Applying transformation to column: {column} with lambda: {lambda_param}\")\n","    \n","    print(f\"Number of rows before transformation: {len(df)}\")\n","    print(f\"Number of rows after removing NaN values: {len(df_copy)}\")\n","    \n","    return df_copy, lambda_param\n","\n","# Apply Box-Cox transformation to train data and store lambda values\n","for column in skewed_columns:\n","    transformed_train_data, lambda_params[column] = box_cox_transform(train, column)\n","    # Update only the new transformed column in the original dataframe\n","    train[f'{column}_boxcox'] = transformed_train_data[f'{column}_boxcox']\n","\n","# Apply the same transformation to test data using stored lambda values\n","for column in skewed_columns:\n","    transformed_test_data, _ = box_cox_transform(test, column, lambda_param=lambda_params[column])\n","    # Update only the new transformed column in the original dataframe\n","    test[f'{column}_boxcox'] = transformed_test_data[f'{column}_boxcox']\n","\n","# Function to handle infinite values\n","def replace_inf_with_max(df):\n","    for column in df.columns:\n","        if df[column].dtype == 'float64':\n","            max_value = df[column][~np.isinf(df[column])].max()\n","            df[column] = df[column].replace([np.inf, -np.inf], max_value)\n","    return df\n","\n","# Replace infinite values with the maximum non-infinite value in each column\n","train_data = replace_inf_with_max(train)\n","test_data = replace_inf_with_max(test)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Basic_Demos-Enroll_Season</th>\n","      <th>Basic_Demos-Age</th>\n","      <th>Basic_Demos-Sex</th>\n","      <th>CGAS-Season</th>\n","      <th>CGAS-CGAS_Score</th>\n","      <th>Physical-Season</th>\n","      <th>Physical-BMI</th>\n","      <th>Physical-Height</th>\n","      <th>Physical-Weight</th>\n","      <th>Physical-Waist_Circumference</th>\n","      <th>Physical-Diastolic_BP</th>\n","      <th>Physical-HeartRate</th>\n","      <th>Physical-Systolic_BP</th>\n","      <th>Fitness_Endurance-Season</th>\n","      <th>Fitness_Endurance-Max_Stage</th>\n","      <th>Fitness_Endurance-Time_Mins</th>\n","      <th>Fitness_Endurance-Time_Sec</th>\n","      <th>FGC-Season</th>\n","      <th>FGC-FGC_CU</th>\n","      <th>FGC-FGC_CU_Zone</th>\n","      <th>FGC-FGC_GSND</th>\n","      <th>FGC-FGC_GSND_Zone</th>\n","      <th>FGC-FGC_GSD</th>\n","      <th>FGC-FGC_GSD_Zone</th>\n","      <th>FGC-FGC_PU</th>\n","      <th>FGC-FGC_PU_Zone</th>\n","      <th>FGC-FGC_SRL</th>\n","      <th>FGC-FGC_SRL_Zone</th>\n","      <th>FGC-FGC_SRR</th>\n","      <th>FGC-FGC_SRR_Zone</th>\n","      <th>FGC-FGC_TL</th>\n","      <th>FGC-FGC_TL_Zone</th>\n","      <th>BIA-Season</th>\n","      <th>BIA-BIA_Activity_Level_num</th>\n","      <th>BIA-BIA_BMC</th>\n","      <th>BIA-BIA_BMI</th>\n","      <th>BIA-BIA_BMR</th>\n","      <th>BIA-BIA_DEE</th>\n","      <th>BIA-BIA_ECW</th>\n","      <th>BIA-BIA_FFM</th>\n","      <th>BIA-BIA_FFMI</th>\n","      <th>BIA-BIA_FMI</th>\n","      <th>BIA-BIA_Fat</th>\n","      <th>BIA-BIA_Frame_num</th>\n","      <th>BIA-BIA_ICW</th>\n","      <th>BIA-BIA_LDM</th>\n","      <th>BIA-BIA_LST</th>\n","      <th>BIA-BIA_SMM</th>\n","      <th>BIA-BIA_TBW</th>\n","      <th>PAQ_A-Season</th>\n","      <th>PAQ_A-PAQ_A_Total</th>\n","      <th>PAQ_C-Season</th>\n","      <th>PAQ_C-PAQ_C_Total</th>\n","      <th>PCIAT-Season</th>\n","      <th>PCIAT-PCIAT_01</th>\n","      <th>PCIAT-PCIAT_02</th>\n","      <th>PCIAT-PCIAT_03</th>\n","      <th>PCIAT-PCIAT_04</th>\n","      <th>PCIAT-PCIAT_05</th>\n","      <th>PCIAT-PCIAT_06</th>\n","      <th>PCIAT-PCIAT_07</th>\n","      <th>PCIAT-PCIAT_08</th>\n","      <th>PCIAT-PCIAT_09</th>\n","      <th>PCIAT-PCIAT_10</th>\n","      <th>PCIAT-PCIAT_11</th>\n","      <th>PCIAT-PCIAT_12</th>\n","      <th>PCIAT-PCIAT_13</th>\n","      <th>PCIAT-PCIAT_14</th>\n","      <th>PCIAT-PCIAT_15</th>\n","      <th>PCIAT-PCIAT_16</th>\n","      <th>PCIAT-PCIAT_17</th>\n","      <th>PCIAT-PCIAT_18</th>\n","      <th>PCIAT-PCIAT_19</th>\n","      <th>PCIAT-PCIAT_20</th>\n","      <th>PCIAT-PCIAT_Total</th>\n","      <th>SDS-Season</th>\n","      <th>SDS-SDS_Total_Raw</th>\n","      <th>SDS-SDS_Total_T</th>\n","      <th>PreInt_EduHx-Season</th>\n","      <th>PreInt_EduHx-computerinternet_hoursday</th>\n","      <th>sii</th>\n","      <th>feature_0</th>\n","      <th>feature_1</th>\n","      <th>feature_2</th>\n","      <th>feature_3</th>\n","      <th>feature_4</th>\n","      <th>feature_5</th>\n","      <th>feature_6</th>\n","      <th>feature_7</th>\n","      <th>feature_8</th>\n","      <th>feature_9</th>\n","      <th>feature_10</th>\n","      <th>feature_11</th>\n","      <th>feature_12</th>\n","      <th>feature_13</th>\n","      <th>feature_14</th>\n","      <th>feature_15</th>\n","      <th>feature_16</th>\n","      <th>feature_17</th>\n","      <th>feature_18</th>\n","      <th>feature_19</th>\n","      <th>feature_20</th>\n","      <th>feature_21</th>\n","      <th>feature_22</th>\n","      <th>feature_23</th>\n","      <th>feature_24</th>\n","      <th>feature_25</th>\n","      <th>feature_26</th>\n","      <th>feature_27</th>\n","      <th>feature_28</th>\n","      <th>feature_29</th>\n","      <th>feature_30</th>\n","      <th>feature_31</th>\n","      <th>feature_32</th>\n","      <th>feature_33</th>\n","      <th>feature_34</th>\n","      <th>feature_35</th>\n","      <th>feature_36</th>\n","      <th>feature_37</th>\n","      <th>feature_38</th>\n","      <th>feature_39</th>\n","      <th>feature_40</th>\n","      <th>feature_41</th>\n","      <th>feature_42</th>\n","      <th>feature_43</th>\n","      <th>feature_44</th>\n","      <th>feature_45</th>\n","      <th>feature_46</th>\n","      <th>feature_47</th>\n","      <th>feature_48</th>\n","      <th>feature_49</th>\n","      <th>feature_50</th>\n","      <th>feature_51</th>\n","      <th>feature_52</th>\n","      <th>feature_53</th>\n","      <th>feature_54</th>\n","      <th>feature_55</th>\n","      <th>feature_56</th>\n","      <th>feature_57</th>\n","      <th>feature_58</th>\n","      <th>feature_59</th>\n","      <th>feature_60</th>\n","      <th>feature_61</th>\n","      <th>feature_62</th>\n","      <th>feature_63</th>\n","      <th>feature_64</th>\n","      <th>feature_65</th>\n","      <th>feature_66</th>\n","      <th>feature_67</th>\n","      <th>feature_68</th>\n","      <th>feature_69</th>\n","      <th>feature_70</th>\n","      <th>feature_71</th>\n","      <th>feature_72</th>\n","      <th>feature_73</th>\n","      <th>feature_74</th>\n","      <th>feature_75</th>\n","      <th>feature_76</th>\n","      <th>feature_77</th>\n","      <th>feature_78</th>\n","      <th>feature_79</th>\n","      <th>feature_80</th>\n","      <th>feature_81</th>\n","      <th>feature_82</th>\n","      <th>feature_83</th>\n","      <th>feature_84</th>\n","      <th>feature_85</th>\n","      <th>feature_86</th>\n","      <th>feature_87</th>\n","      <th>feature_88</th>\n","      <th>feature_89</th>\n","      <th>feature_90</th>\n","      <th>feature_91</th>\n","      <th>feature_92</th>\n","      <th>feature_93</th>\n","      <th>feature_94</th>\n","      <th>feature_95</th>\n","      <th>BIA-BIA_BMC_boxcox</th>\n","      <th>BIA-BIA_BMR_boxcox</th>\n","      <th>BIA-BIA_DEE_boxcox</th>\n","      <th>BIA-BIA_ECW_boxcox</th>\n","      <th>BIA-BIA_Fat_boxcox</th>\n","      <th>BIA-BIA_FFM_boxcox</th>\n","      <th>BIA-BIA_FFMI_boxcox</th>\n","      <th>BIA-BIA_FMI_boxcox</th>\n","      <th>BIA-BIA_ICW_boxcox</th>\n","      <th>BIA-BIA_LDM_boxcox</th>\n","      <th>BIA-BIA_LST_boxcox</th>\n","      <th>BIA-BIA_TBW_boxcox</th>\n","      <th>CGAS-CGAS_Score_boxcox</th>\n","      <th>feature_23_boxcox</th>\n","      <th>feature_35_boxcox</th>\n","      <th>feature_38_boxcox</th>\n","      <th>feature_40_boxcox</th>\n","      <th>feature_47_boxcox</th>\n","      <th>feature_54_boxcox</th>\n","      <th>feature_66_boxcox</th>\n","      <th>feature_78_boxcox</th>\n","      <th>feature_80_boxcox</th>\n","      <th>feature_88_boxcox</th>\n","      <th>feature_90_boxcox</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Fall</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>Winter</td>\n","      <td>51.0</td>\n","      <td>Fall</td>\n","      <td>16.877316</td>\n","      <td>46.00</td>\n","      <td>50.8</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Fall</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.0</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","      <td>1.0</td>\n","      <td>Fall</td>\n","      <td>2.0</td>\n","      <td>2.66855</td>\n","      <td>16.8792</td>\n","      <td>932.498</td>\n","      <td>1492.00</td>\n","      <td>8.25598</td>\n","      <td>41.5862</td>\n","      <td>13.8177</td>\n","      <td>3.06143</td>\n","      <td>9.21377</td>\n","      <td>1.0</td>\n","      <td>24.4349</td>\n","      <td>8.89536</td>\n","      <td>38.9177</td>\n","      <td>19.5413</td>\n","      <td>32.6909</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Fall</td>\n","      <td>5.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>55.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Fall</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.795346</td>\n","      <td>0.494067</td>\n","      <td>1.013221</td>\n","      <td>1.877806</td>\n","      <td>6.776256e+107</td>\n","      <td>1.210694</td>\n","      <td>0.378516</td>\n","      <td>1.308325e+13</td>\n","      <td>0.836481</td>\n","      <td>1.443716</td>\n","      <td>1.461227</td>\n","      <td>1.296982</td>\n","      <td>2.887303</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Summer</td>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Fall</td>\n","      <td>14.035590</td>\n","      <td>48.00</td>\n","      <td>46.0</td>\n","      <td>22.0</td>\n","      <td>75.0</td>\n","      <td>70.0</td>\n","      <td>122.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Fall</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>1.0</td>\n","      <td>11.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>Winter</td>\n","      <td>2.0</td>\n","      <td>2.57949</td>\n","      <td>14.0371</td>\n","      <td>936.656</td>\n","      <td>1498.65</td>\n","      <td>6.01993</td>\n","      <td>42.0291</td>\n","      <td>12.8254</td>\n","      <td>1.21172</td>\n","      <td>3.97085</td>\n","      <td>1.0</td>\n","      <td>21.0352</td>\n","      <td>14.97400</td>\n","      <td>39.4497</td>\n","      <td>15.4107</td>\n","      <td>27.0552</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Fall</td>\n","      <td>2.340</td>\n","      <td>Fall</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>Fall</td>\n","      <td>46.0</td>\n","      <td>64.0</td>\n","      <td>Summer</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.791257</td>\n","      <td>0.494067</td>\n","      <td>1.013225</td>\n","      <td>1.624546</td>\n","      <td>6.664675e+107</td>\n","      <td>1.211268</td>\n","      <td>0.378436</td>\n","      <td>1.236179e+13</td>\n","      <td>0.832549</td>\n","      <td>1.635205</td>\n","      <td>1.462669</td>\n","      <td>1.279664</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Summer</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>Fall</td>\n","      <td>71.0</td>\n","      <td>Fall</td>\n","      <td>16.648696</td>\n","      <td>56.50</td>\n","      <td>75.6</td>\n","      <td>NaN</td>\n","      <td>65.0</td>\n","      <td>94.0</td>\n","      <td>117.0</td>\n","      <td>Fall</td>\n","      <td>5.0</td>\n","      <td>7.0</td>\n","      <td>33.0</td>\n","      <td>Fall</td>\n","      <td>20.0</td>\n","      <td>1.0</td>\n","      <td>10.2</td>\n","      <td>1.0</td>\n","      <td>14.7</td>\n","      <td>2.0</td>\n","      <td>7.0</td>\n","      <td>1.0</td>\n","      <td>10.0</td>\n","      <td>1.0</td>\n","      <td>10.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Summer</td>\n","      <td>2.170</td>\n","      <td>Fall</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>28.0</td>\n","      <td>Fall</td>\n","      <td>38.0</td>\n","      <td>54.0</td>\n","      <td>Summer</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3.054867</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Winter</td>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>Fall</td>\n","      <td>71.0</td>\n","      <td>Summer</td>\n","      <td>18.292347</td>\n","      <td>56.00</td>\n","      <td>81.6</td>\n","      <td>NaN</td>\n","      <td>60.0</td>\n","      <td>97.0</td>\n","      <td>117.0</td>\n","      <td>Summer</td>\n","      <td>6.0</td>\n","      <td>9.0</td>\n","      <td>37.0</td>\n","      <td>Summer</td>\n","      <td>18.0</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>7.0</td>\n","      <td>0.0</td>\n","      <td>7.0</td>\n","      <td>0.0</td>\n","      <td>7.0</td>\n","      <td>1.0</td>\n","      <td>Summer</td>\n","      <td>3.0</td>\n","      <td>3.84191</td>\n","      <td>18.2943</td>\n","      <td>1131.430</td>\n","      <td>1923.44</td>\n","      <td>15.59250</td>\n","      <td>62.7757</td>\n","      <td>14.0740</td>\n","      <td>4.22033</td>\n","      <td>18.82430</td>\n","      <td>2.0</td>\n","      <td>30.4041</td>\n","      <td>16.77900</td>\n","      <td>58.9338</td>\n","      <td>26.4798</td>\n","      <td>45.9966</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Winter</td>\n","      <td>2.451</td>\n","      <td>Summer</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>44.0</td>\n","      <td>Summer</td>\n","      <td>31.0</td>\n","      <td>45.0</td>\n","      <td>Winter</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.059962</td>\n","      <td>0.063853</td>\n","      <td>0.061115</td>\n","      <td>0.057507</td>\n","      <td>0.063705</td>\n","      <td>0.058935</td>\n","      <td>0.063302</td>\n","      <td>0.061840</td>\n","      <td>0.066272</td>\n","      <td>0.059864</td>\n","      <td>0.061409</td>\n","      <td>0.061272</td>\n","      <td>0.482902</td>\n","      <td>0.449879</td>\n","      <td>0.477025</td>\n","      <td>0.360325</td>\n","      <td>0.486286</td>\n","      <td>0.000282</td>\n","      <td>0.194878</td>\n","      <td>0.761338</td>\n","      <td>0.527233</td>\n","      <td>0.599881</td>\n","      <td>0.673393</td>\n","      <td>0.202446</td>\n","      <td>0.669721</td>\n","      <td>0.634308</td>\n","      <td>0.545043</td>\n","      <td>0.321878</td>\n","      <td>0.44320</td>\n","      <td>0.000450</td>\n","      <td>0.323264</td>\n","      <td>0.383129</td>\n","      <td>0.430890</td>\n","      <td>0.631043</td>\n","      <td>0.145090</td>\n","      <td>0.218223</td>\n","      <td>0.782131</td>\n","      <td>0.621180</td>\n","      <td>0.988499</td>\n","      <td>0.000006</td>\n","      <td>0.031951</td>\n","      <td>0.000417</td>\n","      <td>0.000496</td>\n","      <td>0.704413</td>\n","      <td>0.010682</td>\n","      <td>0.005581</td>\n","      <td>0.517803</td>\n","      <td>0.201068</td>\n","      <td>0.386576</td>\n","      <td>0.306167</td>\n","      <td>0.229836</td>\n","      <td>0.188153</td>\n","      <td>0.293499</td>\n","      <td>2.761214e-08</td>\n","      <td>0.021041</td>\n","      <td>0.690482</td>\n","      <td>0.523655</td>\n","      <td>0.348302</td>\n","      <td>0.622966</td>\n","      <td>0.187875</td>\n","      <td>0.462905</td>\n","      <td>0.494512</td>\n","      <td>0.448863</td>\n","      <td>0.408981</td>\n","      <td>0.460584</td>\n","      <td>4.503229e-08</td>\n","      <td>0.070129</td>\n","      <td>0.739498</td>\n","      <td>0.635275</td>\n","      <td>0.523313</td>\n","      <td>0.701488</td>\n","      <td>0.199662</td>\n","      <td>0.549184</td>\n","      <td>0.654837</td>\n","      <td>0.668681</td>\n","      <td>0.423425</td>\n","      <td>0.617634</td>\n","      <td>4.158378e-07</td>\n","      <td>0.062502</td>\n","      <td>0.830508</td>\n","      <td>0.581014</td>\n","      <td>0.718914</td>\n","      <td>0.737830</td>\n","      <td>0.199819</td>\n","      <td>0.164361</td>\n","      <td>0.159703</td>\n","      <td>0.198518</td>\n","      <td>0.296697</td>\n","      <td>0.994268</td>\n","      <td>0.000273</td>\n","      <td>0.101636</td>\n","      <td>0.082785</td>\n","      <td>0.983878</td>\n","      <td>0.998519</td>\n","      <td>0.821466</td>\n","      <td>0.222241</td>\n","      <td>1.845723</td>\n","      <td>0.494067</td>\n","      <td>1.013388</td>\n","      <td>2.360996</td>\n","      <td>6.985483e+107</td>\n","      <td>1.229847</td>\n","      <td>0.378534</td>\n","      <td>1.355292e+13</td>\n","      <td>0.841114</td>\n","      <td>1.671884</td>\n","      <td>1.500300</td>\n","      <td>1.322946</td>\n","      <td>3.054867</td>\n","      <td>-0.935339</td>\n","      <td>-1.227946</td>\n","      <td>-0.009383</td>\n","      <td>-5.353568</td>\n","      <td>-1.031675</td>\n","      <td>-5.306281</td>\n","      <td>-2.761255</td>\n","      <td>-1.590256</td>\n","      <td>-1.645198</td>\n","      <td>-0.004695</td>\n","      <td>-0.641349</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Spring</td>\n","      <td>18</td>\n","      <td>1</td>\n","      <td>Summer</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Summer</td>\n","      <td>1.04</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Spring</td>\n","      <td>13</td>\n","      <td>1</td>\n","      <td>Winter</td>\n","      <td>50.0</td>\n","      <td>Summer</td>\n","      <td>22.279952</td>\n","      <td>59.50</td>\n","      <td>112.2</td>\n","      <td>NaN</td>\n","      <td>60.0</td>\n","      <td>73.0</td>\n","      <td>102.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Summer</td>\n","      <td>12.0</td>\n","      <td>0.0</td>\n","      <td>16.5</td>\n","      <td>2.0</td>\n","      <td>17.9</td>\n","      <td>2.0</td>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","      <td>10.0</td>\n","      <td>1.0</td>\n","      <td>11.0</td>\n","      <td>1.0</td>\n","      <td>8.0</td>\n","      <td>0.0</td>\n","      <td>Summer</td>\n","      <td>2.0</td>\n","      <td>4.33036</td>\n","      <td>30.1865</td>\n","      <td>1330.970</td>\n","      <td>1996.45</td>\n","      <td>30.21240</td>\n","      <td>84.0285</td>\n","      <td>16.6877</td>\n","      <td>13.49880</td>\n","      <td>67.97150</td>\n","      <td>2.0</td>\n","      <td>32.9141</td>\n","      <td>20.90200</td>\n","      <td>79.6982</td>\n","      <td>35.3804</td>\n","      <td>63.1265</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Spring</td>\n","      <td>4.110</td>\n","      <td>Summer</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>34.0</td>\n","      <td>Summer</td>\n","      <td>40.0</td>\n","      <td>56.0</td>\n","      <td>Spring</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.545608</td>\n","      <td>0.564775</td>\n","      <td>0.551163</td>\n","      <td>0.546069</td>\n","      <td>0.544490</td>\n","      <td>0.541436</td>\n","      <td>0.538521</td>\n","      <td>0.550583</td>\n","      <td>0.549428</td>\n","      <td>0.564151</td>\n","      <td>0.551190</td>\n","      <td>0.557732</td>\n","      <td>0.534399</td>\n","      <td>0.466696</td>\n","      <td>0.247047</td>\n","      <td>0.111138</td>\n","      <td>0.275329</td>\n","      <td>0.651414</td>\n","      <td>0.105707</td>\n","      <td>0.395468</td>\n","      <td>0.272272</td>\n","      <td>0.588416</td>\n","      <td>0.686763</td>\n","      <td>0.257021</td>\n","      <td>0.381962</td>\n","      <td>0.497780</td>\n","      <td>0.631040</td>\n","      <td>0.161748</td>\n","      <td>0.60355</td>\n","      <td>0.860121</td>\n","      <td>0.166548</td>\n","      <td>0.677725</td>\n","      <td>0.706236</td>\n","      <td>0.727261</td>\n","      <td>0.212168</td>\n","      <td>0.156329</td>\n","      <td>0.768742</td>\n","      <td>0.599602</td>\n","      <td>0.984312</td>\n","      <td>0.000559</td>\n","      <td>0.040660</td>\n","      <td>0.000027</td>\n","      <td>0.000020</td>\n","      <td>0.040235</td>\n","      <td>0.000006</td>\n","      <td>0.001475</td>\n","      <td>0.502317</td>\n","      <td>0.256872</td>\n","      <td>0.643131</td>\n","      <td>0.439047</td>\n","      <td>0.076362</td>\n","      <td>0.022589</td>\n","      <td>0.091091</td>\n","      <td>4.402062e-02</td>\n","      <td>0.046633</td>\n","      <td>0.365689</td>\n","      <td>0.190693</td>\n","      <td>0.188637</td>\n","      <td>0.567003</td>\n","      <td>0.254875</td>\n","      <td>0.510128</td>\n","      <td>0.488092</td>\n","      <td>0.153818</td>\n","      <td>0.073950</td>\n","      <td>0.203218</td>\n","      <td>9.812259e-01</td>\n","      <td>0.070672</td>\n","      <td>0.315606</td>\n","      <td>0.410554</td>\n","      <td>0.480558</td>\n","      <td>0.677895</td>\n","      <td>0.244116</td>\n","      <td>0.408820</td>\n","      <td>0.574972</td>\n","      <td>0.367852</td>\n","      <td>0.082781</td>\n","      <td>0.374608</td>\n","      <td>9.999955e-01</td>\n","      <td>0.055663</td>\n","      <td>0.465196</td>\n","      <td>0.552358</td>\n","      <td>0.821534</td>\n","      <td>0.740147</td>\n","      <td>0.260989</td>\n","      <td>0.158272</td>\n","      <td>0.180590</td>\n","      <td>0.207435</td>\n","      <td>0.310252</td>\n","      <td>0.991287</td>\n","      <td>1.000000</td>\n","      <td>0.101612</td>\n","      <td>0.095628</td>\n","      <td>0.999995</td>\n","      <td>0.999714</td>\n","      <td>0.793989</td>\n","      <td>0.257868</td>\n","      <td>1.864977</td>\n","      <td>0.494067</td>\n","      <td>1.013409</td>\n","      <td>2.828091</td>\n","      <td>8.156581e+107</td>\n","      <td>1.240129</td>\n","      <td>0.378661</td>\n","      <td>1.784642e+13</td>\n","      <td>0.842524</td>\n","      <td>1.738032</td>\n","      <td>1.523098</td>\n","      <td>1.342026</td>\n","      <td>2.876979</td>\n","      <td>-0.855591</td>\n","      <td>-1.431325</td>\n","      <td>-0.011925</td>\n","      <td>-4.818598</td>\n","      <td>-0.930557</td>\n","      <td>-3.935685</td>\n","      <td>-2.752938</td>\n","      <td>-1.623008</td>\n","      <td>-2.022604</td>\n","      <td>-0.006468</td>\n","      <td>-0.641357</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Fall</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Fall</td>\n","      <td>19.660760</td>\n","      <td>55.00</td>\n","      <td>84.6</td>\n","      <td>NaN</td>\n","      <td>123.0</td>\n","      <td>83.0</td>\n","      <td>163.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Fall</td>\n","      <td>9.0</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>1.0</td>\n","      <td>11.0</td>\n","      <td>1.0</td>\n","      <td>11.0</td>\n","      <td>1.0</td>\n","      <td>Fall</td>\n","      <td>2.0</td>\n","      <td>3.78271</td>\n","      <td>19.6629</td>\n","      <td>1135.860</td>\n","      <td>1817.38</td>\n","      <td>16.32750</td>\n","      <td>63.2470</td>\n","      <td>14.7000</td>\n","      <td>4.96291</td>\n","      <td>21.35300</td>\n","      <td>2.0</td>\n","      <td>30.8936</td>\n","      <td>16.02590</td>\n","      <td>59.4643</td>\n","      <td>26.1957</td>\n","      <td>47.2211</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Winter</td>\n","      <td>3.670</td>\n","      <td>Winter</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>20.0</td>\n","      <td>Winter</td>\n","      <td>27.0</td>\n","      <td>40.0</td>\n","      <td>Fall</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.843325</td>\n","      <td>0.494067</td>\n","      <td>1.013354</td>\n","      <td>2.394666</td>\n","      <td>7.041561e+107</td>\n","      <td>1.230141</td>\n","      <td>0.378572</td>\n","      <td>1.386121e+13</td>\n","      <td>0.841408</td>\n","      <td>1.657290</td>\n","      <td>1.501039</td>\n","      <td>1.324696</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Fall</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Fall</td>\n","      <td>16.861286</td>\n","      <td>59.25</td>\n","      <td>84.2</td>\n","      <td>27.0</td>\n","      <td>71.0</td>\n","      <td>90.0</td>\n","      <td>116.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Fall</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>12.6</td>\n","      <td>2.0</td>\n","      <td>11.1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>Fall</td>\n","      <td>3.0</td>\n","      <td>4.05726</td>\n","      <td>16.8631</td>\n","      <td>1180.040</td>\n","      <td>1888.06</td>\n","      <td>21.94000</td>\n","      <td>67.9527</td>\n","      <td>13.6092</td>\n","      <td>3.25395</td>\n","      <td>16.24740</td>\n","      <td>2.0</td>\n","      <td>28.5367</td>\n","      <td>17.47600</td>\n","      <td>63.8954</td>\n","      <td>28.7680</td>\n","      <td>50.4767</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Fall</td>\n","      <td>1.270</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Fall</td>\n","      <td>2.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.854326</td>\n","      <td>0.494067</td>\n","      <td>1.013377</td>\n","      <td>2.606525</td>\n","      <td>6.928780e+107</td>\n","      <td>1.232877</td>\n","      <td>0.378501</td>\n","      <td>1.316032e+13</td>\n","      <td>0.839890</td>\n","      <td>1.684591</td>\n","      <td>1.506823</td>\n","      <td>1.328996</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Summer</td>\n","      <td>15</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Spring</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Spring</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Summer</td>\n","      <td>2.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Summer</td>\n","      <td>19</td>\n","      <td>1</td>\n","      <td>Summer</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Basic_Demos-Enroll_Season  Basic_Demos-Age  Basic_Demos-Sex CGAS-Season  \\\n","0                      Fall                5                0      Winter   \n","1                    Summer                9                0         NaN   \n","2                    Summer               10                1        Fall   \n","3                    Winter                9                0        Fall   \n","4                    Spring               18                1      Summer   \n","5                    Spring               13                1      Winter   \n","6                      Fall               10                0         NaN   \n","7                      Fall               10                1         NaN   \n","8                    Summer               15                0         NaN   \n","9                    Summer               19                1      Summer   \n","\n","   CGAS-CGAS_Score Physical-Season  Physical-BMI  Physical-Height  \\\n","0             51.0            Fall     16.877316            46.00   \n","1              NaN            Fall     14.035590            48.00   \n","2             71.0            Fall     16.648696            56.50   \n","3             71.0          Summer     18.292347            56.00   \n","4              NaN             NaN           NaN              NaN   \n","5             50.0          Summer     22.279952            59.50   \n","6              NaN            Fall     19.660760            55.00   \n","7              NaN            Fall     16.861286            59.25   \n","8              NaN          Spring           NaN              NaN   \n","9              NaN             NaN           NaN              NaN   \n","\n","   Physical-Weight  Physical-Waist_Circumference  Physical-Diastolic_BP  \\\n","0             50.8                           NaN                    NaN   \n","1             46.0                          22.0                   75.0   \n","2             75.6                           NaN                   65.0   \n","3             81.6                           NaN                   60.0   \n","4              NaN                           NaN                    NaN   \n","5            112.2                           NaN                   60.0   \n","6             84.6                           NaN                  123.0   \n","7             84.2                          27.0                   71.0   \n","8              NaN                           NaN                    NaN   \n","9              NaN                           NaN                    NaN   \n","\n","   Physical-HeartRate  Physical-Systolic_BP Fitness_Endurance-Season  \\\n","0                 NaN                   NaN                      NaN   \n","1                70.0                 122.0                      NaN   \n","2                94.0                 117.0                     Fall   \n","3                97.0                 117.0                   Summer   \n","4                 NaN                   NaN                      NaN   \n","5                73.0                 102.0                      NaN   \n","6                83.0                 163.0                      NaN   \n","7                90.0                 116.0                      NaN   \n","8                 NaN                   NaN                      NaN   \n","9                 NaN                   NaN                      NaN   \n","\n","   Fitness_Endurance-Max_Stage  Fitness_Endurance-Time_Mins  \\\n","0                          NaN                          NaN   \n","1                          NaN                          NaN   \n","2                          5.0                          7.0   \n","3                          6.0                          9.0   \n","4                          NaN                          NaN   \n","5                          NaN                          NaN   \n","6                          NaN                          NaN   \n","7                          NaN                          NaN   \n","8                          NaN                          NaN   \n","9                          NaN                          NaN   \n","\n","   Fitness_Endurance-Time_Sec FGC-Season  FGC-FGC_CU  FGC-FGC_CU_Zone  \\\n","0                         NaN       Fall         0.0              0.0   \n","1                         NaN       Fall         3.0              0.0   \n","2                        33.0       Fall        20.0              1.0   \n","3                        37.0     Summer        18.0              1.0   \n","4                         NaN        NaN         NaN              NaN   \n","5                         NaN     Summer        12.0              0.0   \n","6                         NaN       Fall         9.0              1.0   \n","7                         NaN       Fall         0.0              0.0   \n","8                         NaN     Spring         NaN              NaN   \n","9                         NaN        NaN         NaN              NaN   \n","\n","   FGC-FGC_GSND  FGC-FGC_GSND_Zone  FGC-FGC_GSD  FGC-FGC_GSD_Zone  FGC-FGC_PU  \\\n","0           NaN                NaN          NaN               NaN         0.0   \n","1           NaN                NaN          NaN               NaN         5.0   \n","2          10.2                1.0         14.7               2.0         7.0   \n","3           NaN                NaN          NaN               NaN         5.0   \n","4           NaN                NaN          NaN               NaN         NaN   \n","5          16.5                2.0         17.9               2.0         6.0   \n","6           NaN                NaN          NaN               NaN         2.0   \n","7          12.6                2.0         11.1               1.0         0.0   \n","8           NaN                NaN          NaN               NaN         NaN   \n","9           NaN                NaN          NaN               NaN         NaN   \n","\n","   FGC-FGC_PU_Zone  FGC-FGC_SRL  FGC-FGC_SRL_Zone  FGC-FGC_SRR  \\\n","0              0.0          7.0               0.0          6.0   \n","1              0.0         11.0               1.0         11.0   \n","2              1.0         10.0               1.0         10.0   \n","3              0.0          7.0               0.0          7.0   \n","4              NaN          NaN               NaN          NaN   \n","5              0.0         10.0               1.0         11.0   \n","6              0.0         11.0               1.0         11.0   \n","7              0.0          0.0               0.0          0.0   \n","8              NaN          NaN               NaN          NaN   \n","9              NaN          NaN               NaN          NaN   \n","\n","   FGC-FGC_SRR_Zone  FGC-FGC_TL  FGC-FGC_TL_Zone BIA-Season  \\\n","0               0.0         6.0              1.0       Fall   \n","1               1.0         3.0              0.0     Winter   \n","2               1.0         5.0              0.0        NaN   \n","3               0.0         7.0              1.0     Summer   \n","4               NaN         NaN              NaN        NaN   \n","5               1.0         8.0              0.0     Summer   \n","6               1.0        11.0              1.0       Fall   \n","7               0.0         4.0              0.0       Fall   \n","8               NaN         NaN              NaN        NaN   \n","9               NaN         NaN              NaN        NaN   \n","\n","   BIA-BIA_Activity_Level_num  BIA-BIA_BMC  BIA-BIA_BMI  BIA-BIA_BMR  \\\n","0                         2.0      2.66855      16.8792      932.498   \n","1                         2.0      2.57949      14.0371      936.656   \n","2                         NaN          NaN          NaN          NaN   \n","3                         3.0      3.84191      18.2943     1131.430   \n","4                         NaN          NaN          NaN          NaN   \n","5                         2.0      4.33036      30.1865     1330.970   \n","6                         2.0      3.78271      19.6629     1135.860   \n","7                         3.0      4.05726      16.8631     1180.040   \n","8                         NaN          NaN          NaN          NaN   \n","9                         NaN          NaN          NaN          NaN   \n","\n","   BIA-BIA_DEE  BIA-BIA_ECW  BIA-BIA_FFM  BIA-BIA_FFMI  BIA-BIA_FMI  \\\n","0      1492.00      8.25598      41.5862       13.8177      3.06143   \n","1      1498.65      6.01993      42.0291       12.8254      1.21172   \n","2          NaN          NaN          NaN           NaN          NaN   \n","3      1923.44     15.59250      62.7757       14.0740      4.22033   \n","4          NaN          NaN          NaN           NaN          NaN   \n","5      1996.45     30.21240      84.0285       16.6877     13.49880   \n","6      1817.38     16.32750      63.2470       14.7000      4.96291   \n","7      1888.06     21.94000      67.9527       13.6092      3.25395   \n","8          NaN          NaN          NaN           NaN          NaN   \n","9          NaN          NaN          NaN           NaN          NaN   \n","\n","   BIA-BIA_Fat  BIA-BIA_Frame_num  BIA-BIA_ICW  BIA-BIA_LDM  BIA-BIA_LST  \\\n","0      9.21377                1.0      24.4349      8.89536      38.9177   \n","1      3.97085                1.0      21.0352     14.97400      39.4497   \n","2          NaN                NaN          NaN          NaN          NaN   \n","3     18.82430                2.0      30.4041     16.77900      58.9338   \n","4          NaN                NaN          NaN          NaN          NaN   \n","5     67.97150                2.0      32.9141     20.90200      79.6982   \n","6     21.35300                2.0      30.8936     16.02590      59.4643   \n","7     16.24740                2.0      28.5367     17.47600      63.8954   \n","8          NaN                NaN          NaN          NaN          NaN   \n","9          NaN                NaN          NaN          NaN          NaN   \n","\n","   BIA-BIA_SMM  BIA-BIA_TBW PAQ_A-Season  PAQ_A-PAQ_A_Total PAQ_C-Season  \\\n","0      19.5413      32.6909          NaN                NaN          NaN   \n","1      15.4107      27.0552          NaN                NaN         Fall   \n","2          NaN          NaN          NaN                NaN       Summer   \n","3      26.4798      45.9966          NaN                NaN       Winter   \n","4          NaN          NaN       Summer               1.04          NaN   \n","5      35.3804      63.1265          NaN                NaN       Spring   \n","6      26.1957      47.2211          NaN                NaN       Winter   \n","7      28.7680      50.4767          NaN                NaN         Fall   \n","8          NaN          NaN          NaN                NaN          NaN   \n","9          NaN          NaN          NaN                NaN          NaN   \n","\n","   PAQ_C-PAQ_C_Total PCIAT-Season  PCIAT-PCIAT_01  PCIAT-PCIAT_02  \\\n","0                NaN         Fall             5.0             4.0   \n","1              2.340         Fall             0.0             0.0   \n","2              2.170         Fall             5.0             2.0   \n","3              2.451       Summer             4.0             2.0   \n","4                NaN          NaN             NaN             NaN   \n","5              4.110       Summer             3.0             3.0   \n","6              3.670       Winter             1.0             4.0   \n","7              1.270          NaN             NaN             NaN   \n","8                NaN          NaN             NaN             NaN   \n","9                NaN          NaN             NaN             NaN   \n","\n","   PCIAT-PCIAT_03  PCIAT-PCIAT_04  PCIAT-PCIAT_05  PCIAT-PCIAT_06  \\\n","0             4.0             0.0             4.0             0.0   \n","1             0.0             0.0             0.0             0.0   \n","2             2.0             1.0             2.0             1.0   \n","3             4.0             0.0             5.0             1.0   \n","4             NaN             NaN             NaN             NaN   \n","5             3.0             0.0             2.0             1.0   \n","6             1.0             0.0             2.0             1.0   \n","7             NaN             NaN             NaN             NaN   \n","8             NaN             NaN             NaN             NaN   \n","9             NaN             NaN             NaN             NaN   \n","\n","   PCIAT-PCIAT_07  PCIAT-PCIAT_08  PCIAT-PCIAT_09  PCIAT-PCIAT_10  \\\n","0             0.0             4.0             0.0             0.0   \n","1             0.0             0.0             0.0             0.0   \n","2             1.0             2.0             1.0             1.0   \n","3             0.0             3.0             2.0             2.0   \n","4             NaN             NaN             NaN             NaN   \n","5             0.0             2.0             2.0             1.0   \n","6             0.0             1.0             0.0             0.0   \n","7             NaN             NaN             NaN             NaN   \n","8             NaN             NaN             NaN             NaN   \n","9             NaN             NaN             NaN             NaN   \n","\n","   PCIAT-PCIAT_11  PCIAT-PCIAT_12  PCIAT-PCIAT_13  PCIAT-PCIAT_14  \\\n","0             4.0             0.0             4.0             4.0   \n","1             0.0             0.0             0.0             0.0   \n","2             1.0             0.0             1.0             1.0   \n","3             3.0             0.0             3.0             0.0   \n","4             NaN             NaN             NaN             NaN   \n","5             0.0             1.0             3.0             3.0   \n","6             0.0             0.0             0.0             0.0   \n","7             NaN             NaN             NaN             NaN   \n","8             NaN             NaN             NaN             NaN   \n","9             NaN             NaN             NaN             NaN   \n","\n","   PCIAT-PCIAT_15  PCIAT-PCIAT_16  PCIAT-PCIAT_17  PCIAT-PCIAT_18  \\\n","0             4.0             4.0             4.0             4.0   \n","1             0.0             0.0             0.0             0.0   \n","2             1.0             0.0             2.0             2.0   \n","3             0.0             3.0             4.0             3.0   \n","4             NaN             NaN             NaN             NaN   \n","5             2.0             1.0             3.0             1.0   \n","6             0.0             4.0             1.0             4.0   \n","7             NaN             NaN             NaN             NaN   \n","8             NaN             NaN             NaN             NaN   \n","9             NaN             NaN             NaN             NaN   \n","\n","   PCIAT-PCIAT_19  PCIAT-PCIAT_20  PCIAT-PCIAT_Total SDS-Season  \\\n","0             2.0             4.0               55.0        NaN   \n","1             0.0             0.0                0.0       Fall   \n","2             1.0             1.0               28.0       Fall   \n","3             4.0             1.0               44.0     Summer   \n","4             NaN             NaN                NaN        NaN   \n","5             2.0             1.0               34.0     Summer   \n","6             1.0             0.0               20.0     Winter   \n","7             NaN             NaN                NaN        NaN   \n","8             NaN             NaN                NaN        NaN   \n","9             NaN             NaN                NaN        NaN   \n","\n","   SDS-SDS_Total_Raw  SDS-SDS_Total_T PreInt_EduHx-Season  \\\n","0                NaN              NaN                Fall   \n","1               46.0             64.0              Summer   \n","2               38.0             54.0              Summer   \n","3               31.0             45.0              Winter   \n","4                NaN              NaN                 NaN   \n","5               40.0             56.0              Spring   \n","6               27.0             40.0                Fall   \n","7                NaN              NaN                Fall   \n","8                NaN              NaN              Summer   \n","9                NaN              NaN                 NaN   \n","\n","   PreInt_EduHx-computerinternet_hoursday  sii  feature_0  feature_1  \\\n","0                                     3.0  2.0        NaN        NaN   \n","1                                     0.0  0.0        NaN        NaN   \n","2                                     2.0  0.0        NaN        NaN   \n","3                                     0.0  1.0   0.059962   0.063853   \n","4                                     NaN  NaN        NaN        NaN   \n","5                                     0.0  1.0   0.545608   0.564775   \n","6                                     3.0  0.0        NaN        NaN   \n","7                                     2.0  NaN        NaN        NaN   \n","8                                     2.0  NaN        NaN        NaN   \n","9                                     NaN  NaN        NaN        NaN   \n","\n","   feature_2  feature_3  feature_4  feature_5  feature_6  feature_7  \\\n","0        NaN        NaN        NaN        NaN        NaN        NaN   \n","1        NaN        NaN        NaN        NaN        NaN        NaN   \n","2        NaN        NaN        NaN        NaN        NaN        NaN   \n","3   0.061115   0.057507   0.063705   0.058935   0.063302   0.061840   \n","4        NaN        NaN        NaN        NaN        NaN        NaN   \n","5   0.551163   0.546069   0.544490   0.541436   0.538521   0.550583   \n","6        NaN        NaN        NaN        NaN        NaN        NaN   \n","7        NaN        NaN        NaN        NaN        NaN        NaN   \n","8        NaN        NaN        NaN        NaN        NaN        NaN   \n","9        NaN        NaN        NaN        NaN        NaN        NaN   \n","\n","   feature_8  feature_9  feature_10  feature_11  feature_12  feature_13  \\\n","0        NaN        NaN         NaN         NaN         NaN         NaN   \n","1        NaN        NaN         NaN         NaN         NaN         NaN   \n","2        NaN        NaN         NaN         NaN         NaN         NaN   \n","3   0.066272   0.059864    0.061409    0.061272    0.482902    0.449879   \n","4        NaN        NaN         NaN         NaN         NaN         NaN   \n","5   0.549428   0.564151    0.551190    0.557732    0.534399    0.466696   \n","6        NaN        NaN         NaN         NaN         NaN         NaN   \n","7        NaN        NaN         NaN         NaN         NaN         NaN   \n","8        NaN        NaN         NaN         NaN         NaN         NaN   \n","9        NaN        NaN         NaN         NaN         NaN         NaN   \n","\n","   feature_14  feature_15  feature_16  feature_17  feature_18  feature_19  \\\n","0         NaN         NaN         NaN         NaN         NaN         NaN   \n","1         NaN         NaN         NaN         NaN         NaN         NaN   \n","2         NaN         NaN         NaN         NaN         NaN         NaN   \n","3    0.477025    0.360325    0.486286    0.000282    0.194878    0.761338   \n","4         NaN         NaN         NaN         NaN         NaN         NaN   \n","5    0.247047    0.111138    0.275329    0.651414    0.105707    0.395468   \n","6         NaN         NaN         NaN         NaN         NaN         NaN   \n","7         NaN         NaN         NaN         NaN         NaN         NaN   \n","8         NaN         NaN         NaN         NaN         NaN         NaN   \n","9         NaN         NaN         NaN         NaN         NaN         NaN   \n","\n","   feature_20  feature_21  feature_22  feature_23  feature_24  feature_25  \\\n","0         NaN         NaN         NaN         NaN         NaN         NaN   \n","1         NaN         NaN         NaN         NaN         NaN         NaN   \n","2         NaN         NaN         NaN         NaN         NaN         NaN   \n","3    0.527233    0.599881    0.673393    0.202446    0.669721    0.634308   \n","4         NaN         NaN         NaN         NaN         NaN         NaN   \n","5    0.272272    0.588416    0.686763    0.257021    0.381962    0.497780   \n","6         NaN         NaN         NaN         NaN         NaN         NaN   \n","7         NaN         NaN         NaN         NaN         NaN         NaN   \n","8         NaN         NaN         NaN         NaN         NaN         NaN   \n","9         NaN         NaN         NaN         NaN         NaN         NaN   \n","\n","   feature_26  feature_27  feature_28  feature_29  feature_30  feature_31  \\\n","0         NaN         NaN         NaN         NaN         NaN         NaN   \n","1         NaN         NaN         NaN         NaN         NaN         NaN   \n","2         NaN         NaN         NaN         NaN         NaN         NaN   \n","3    0.545043    0.321878     0.44320    0.000450    0.323264    0.383129   \n","4         NaN         NaN         NaN         NaN         NaN         NaN   \n","5    0.631040    0.161748     0.60355    0.860121    0.166548    0.677725   \n","6         NaN         NaN         NaN         NaN         NaN         NaN   \n","7         NaN         NaN         NaN         NaN         NaN         NaN   \n","8         NaN         NaN         NaN         NaN         NaN         NaN   \n","9         NaN         NaN         NaN         NaN         NaN         NaN   \n","\n","   feature_32  feature_33  feature_34  feature_35  feature_36  feature_37  \\\n","0         NaN         NaN         NaN         NaN         NaN         NaN   \n","1         NaN         NaN         NaN         NaN         NaN         NaN   \n","2         NaN         NaN         NaN         NaN         NaN         NaN   \n","3    0.430890    0.631043    0.145090    0.218223    0.782131    0.621180   \n","4         NaN         NaN         NaN         NaN         NaN         NaN   \n","5    0.706236    0.727261    0.212168    0.156329    0.768742    0.599602   \n","6         NaN         NaN         NaN         NaN         NaN         NaN   \n","7         NaN         NaN         NaN         NaN         NaN         NaN   \n","8         NaN         NaN         NaN         NaN         NaN         NaN   \n","9         NaN         NaN         NaN         NaN         NaN         NaN   \n","\n","   feature_38  feature_39  feature_40  feature_41  feature_42  feature_43  \\\n","0         NaN         NaN         NaN         NaN         NaN         NaN   \n","1         NaN         NaN         NaN         NaN         NaN         NaN   \n","2         NaN         NaN         NaN         NaN         NaN         NaN   \n","3    0.988499    0.000006    0.031951    0.000417    0.000496    0.704413   \n","4         NaN         NaN         NaN         NaN         NaN         NaN   \n","5    0.984312    0.000559    0.040660    0.000027    0.000020    0.040235   \n","6         NaN         NaN         NaN         NaN         NaN         NaN   \n","7         NaN         NaN         NaN         NaN         NaN         NaN   \n","8         NaN         NaN         NaN         NaN         NaN         NaN   \n","9         NaN         NaN         NaN         NaN         NaN         NaN   \n","\n","   feature_44  feature_45  feature_46  feature_47  feature_48  feature_49  \\\n","0         NaN         NaN         NaN         NaN         NaN         NaN   \n","1         NaN         NaN         NaN         NaN         NaN         NaN   \n","2         NaN         NaN         NaN         NaN         NaN         NaN   \n","3    0.010682    0.005581    0.517803    0.201068    0.386576    0.306167   \n","4         NaN         NaN         NaN         NaN         NaN         NaN   \n","5    0.000006    0.001475    0.502317    0.256872    0.643131    0.439047   \n","6         NaN         NaN         NaN         NaN         NaN         NaN   \n","7         NaN         NaN         NaN         NaN         NaN         NaN   \n","8         NaN         NaN         NaN         NaN         NaN         NaN   \n","9         NaN         NaN         NaN         NaN         NaN         NaN   \n","\n","   feature_50  feature_51  feature_52    feature_53  feature_54  feature_55  \\\n","0         NaN         NaN         NaN           NaN         NaN         NaN   \n","1         NaN         NaN         NaN           NaN         NaN         NaN   \n","2         NaN         NaN         NaN           NaN         NaN         NaN   \n","3    0.229836    0.188153    0.293499  2.761214e-08    0.021041    0.690482   \n","4         NaN         NaN         NaN           NaN         NaN         NaN   \n","5    0.076362    0.022589    0.091091  4.402062e-02    0.046633    0.365689   \n","6         NaN         NaN         NaN           NaN         NaN         NaN   \n","7         NaN         NaN         NaN           NaN         NaN         NaN   \n","8         NaN         NaN         NaN           NaN         NaN         NaN   \n","9         NaN         NaN         NaN           NaN         NaN         NaN   \n","\n","   feature_56  feature_57  feature_58  feature_59  feature_60  feature_61  \\\n","0         NaN         NaN         NaN         NaN         NaN         NaN   \n","1         NaN         NaN         NaN         NaN         NaN         NaN   \n","2         NaN         NaN         NaN         NaN         NaN         NaN   \n","3    0.523655    0.348302    0.622966    0.187875    0.462905    0.494512   \n","4         NaN         NaN         NaN         NaN         NaN         NaN   \n","5    0.190693    0.188637    0.567003    0.254875    0.510128    0.488092   \n","6         NaN         NaN         NaN         NaN         NaN         NaN   \n","7         NaN         NaN         NaN         NaN         NaN         NaN   \n","8         NaN         NaN         NaN         NaN         NaN         NaN   \n","9         NaN         NaN         NaN         NaN         NaN         NaN   \n","\n","   feature_62  feature_63  feature_64    feature_65  feature_66  feature_67  \\\n","0         NaN         NaN         NaN           NaN         NaN         NaN   \n","1         NaN         NaN         NaN           NaN         NaN         NaN   \n","2         NaN         NaN         NaN           NaN         NaN         NaN   \n","3    0.448863    0.408981    0.460584  4.503229e-08    0.070129    0.739498   \n","4         NaN         NaN         NaN           NaN         NaN         NaN   \n","5    0.153818    0.073950    0.203218  9.812259e-01    0.070672    0.315606   \n","6         NaN         NaN         NaN           NaN         NaN         NaN   \n","7         NaN         NaN         NaN           NaN         NaN         NaN   \n","8         NaN         NaN         NaN           NaN         NaN         NaN   \n","9         NaN         NaN         NaN           NaN         NaN         NaN   \n","\n","   feature_68  feature_69  feature_70  feature_71  feature_72  feature_73  \\\n","0         NaN         NaN         NaN         NaN         NaN         NaN   \n","1         NaN         NaN         NaN         NaN         NaN         NaN   \n","2         NaN         NaN         NaN         NaN         NaN         NaN   \n","3    0.635275    0.523313    0.701488    0.199662    0.549184    0.654837   \n","4         NaN         NaN         NaN         NaN         NaN         NaN   \n","5    0.410554    0.480558    0.677895    0.244116    0.408820    0.574972   \n","6         NaN         NaN         NaN         NaN         NaN         NaN   \n","7         NaN         NaN         NaN         NaN         NaN         NaN   \n","8         NaN         NaN         NaN         NaN         NaN         NaN   \n","9         NaN         NaN         NaN         NaN         NaN         NaN   \n","\n","   feature_74  feature_75  feature_76    feature_77  feature_78  feature_79  \\\n","0         NaN         NaN         NaN           NaN         NaN         NaN   \n","1         NaN         NaN         NaN           NaN         NaN         NaN   \n","2         NaN         NaN         NaN           NaN         NaN         NaN   \n","3    0.668681    0.423425    0.617634  4.158378e-07    0.062502    0.830508   \n","4         NaN         NaN         NaN           NaN         NaN         NaN   \n","5    0.367852    0.082781    0.374608  9.999955e-01    0.055663    0.465196   \n","6         NaN         NaN         NaN           NaN         NaN         NaN   \n","7         NaN         NaN         NaN           NaN         NaN         NaN   \n","8         NaN         NaN         NaN           NaN         NaN         NaN   \n","9         NaN         NaN         NaN           NaN         NaN         NaN   \n","\n","   feature_80  feature_81  feature_82  feature_83  feature_84  feature_85  \\\n","0         NaN         NaN         NaN         NaN         NaN         NaN   \n","1         NaN         NaN         NaN         NaN         NaN         NaN   \n","2         NaN         NaN         NaN         NaN         NaN         NaN   \n","3    0.581014    0.718914    0.737830    0.199819    0.164361    0.159703   \n","4         NaN         NaN         NaN         NaN         NaN         NaN   \n","5    0.552358    0.821534    0.740147    0.260989    0.158272    0.180590   \n","6         NaN         NaN         NaN         NaN         NaN         NaN   \n","7         NaN         NaN         NaN         NaN         NaN         NaN   \n","8         NaN         NaN         NaN         NaN         NaN         NaN   \n","9         NaN         NaN         NaN         NaN         NaN         NaN   \n","\n","   feature_86  feature_87  feature_88  feature_89  feature_90  feature_91  \\\n","0         NaN         NaN         NaN         NaN         NaN         NaN   \n","1         NaN         NaN         NaN         NaN         NaN         NaN   \n","2         NaN         NaN         NaN         NaN         NaN         NaN   \n","3    0.198518    0.296697    0.994268    0.000273    0.101636    0.082785   \n","4         NaN         NaN         NaN         NaN         NaN         NaN   \n","5    0.207435    0.310252    0.991287    1.000000    0.101612    0.095628   \n","6         NaN         NaN         NaN         NaN         NaN         NaN   \n","7         NaN         NaN         NaN         NaN         NaN         NaN   \n","8         NaN         NaN         NaN         NaN         NaN         NaN   \n","9         NaN         NaN         NaN         NaN         NaN         NaN   \n","\n","   feature_92  feature_93  feature_94  feature_95  BIA-BIA_BMC_boxcox  \\\n","0         NaN         NaN         NaN         NaN            1.795346   \n","1         NaN         NaN         NaN         NaN            1.791257   \n","2         NaN         NaN         NaN         NaN                 NaN   \n","3    0.983878    0.998519    0.821466    0.222241            1.845723   \n","4         NaN         NaN         NaN         NaN                 NaN   \n","5    0.999995    0.999714    0.793989    0.257868            1.864977   \n","6         NaN         NaN         NaN         NaN            1.843325   \n","7         NaN         NaN         NaN         NaN            1.854326   \n","8         NaN         NaN         NaN         NaN                 NaN   \n","9         NaN         NaN         NaN         NaN                 NaN   \n","\n","   BIA-BIA_BMR_boxcox  BIA-BIA_DEE_boxcox  BIA-BIA_ECW_boxcox  \\\n","0            0.494067            1.013221            1.877806   \n","1            0.494067            1.013225            1.624546   \n","2                 NaN                 NaN                 NaN   \n","3            0.494067            1.013388            2.360996   \n","4                 NaN                 NaN                 NaN   \n","5            0.494067            1.013409            2.828091   \n","6            0.494067            1.013354            2.394666   \n","7            0.494067            1.013377            2.606525   \n","8                 NaN                 NaN                 NaN   \n","9                 NaN                 NaN                 NaN   \n","\n","   BIA-BIA_Fat_boxcox  BIA-BIA_FFM_boxcox  BIA-BIA_FFMI_boxcox  \\\n","0       6.776256e+107            1.210694             0.378516   \n","1       6.664675e+107            1.211268             0.378436   \n","2                 NaN                 NaN                  NaN   \n","3       6.985483e+107            1.229847             0.378534   \n","4                 NaN                 NaN                  NaN   \n","5       8.156581e+107            1.240129             0.378661   \n","6       7.041561e+107            1.230141             0.378572   \n","7       6.928780e+107            1.232877             0.378501   \n","8                 NaN                 NaN                  NaN   \n","9                 NaN                 NaN                  NaN   \n","\n","   BIA-BIA_FMI_boxcox  BIA-BIA_ICW_boxcox  BIA-BIA_LDM_boxcox  \\\n","0        1.308325e+13            0.836481            1.443716   \n","1        1.236179e+13            0.832549            1.635205   \n","2                 NaN                 NaN                 NaN   \n","3        1.355292e+13            0.841114            1.671884   \n","4                 NaN                 NaN                 NaN   \n","5        1.784642e+13            0.842524            1.738032   \n","6        1.386121e+13            0.841408            1.657290   \n","7        1.316032e+13            0.839890            1.684591   \n","8                 NaN                 NaN                 NaN   \n","9                 NaN                 NaN                 NaN   \n","\n","   BIA-BIA_LST_boxcox  BIA-BIA_TBW_boxcox  CGAS-CGAS_Score_boxcox  \\\n","0            1.461227            1.296982                2.887303   \n","1            1.462669            1.279664                     NaN   \n","2                 NaN                 NaN                3.054867   \n","3            1.500300            1.322946                3.054867   \n","4                 NaN                 NaN                     NaN   \n","5            1.523098            1.342026                2.876979   \n","6            1.501039            1.324696                     NaN   \n","7            1.506823            1.328996                     NaN   \n","8                 NaN                 NaN                     NaN   \n","9                 NaN                 NaN                     NaN   \n","\n","   feature_23_boxcox  feature_35_boxcox  feature_38_boxcox  feature_40_boxcox  \\\n","0                NaN                NaN                NaN                NaN   \n","1                NaN                NaN                NaN                NaN   \n","2                NaN                NaN                NaN                NaN   \n","3          -0.935339          -1.227946          -0.009383          -5.353568   \n","4                NaN                NaN                NaN                NaN   \n","5          -0.855591          -1.431325          -0.011925          -4.818598   \n","6                NaN                NaN                NaN                NaN   \n","7                NaN                NaN                NaN                NaN   \n","8                NaN                NaN                NaN                NaN   \n","9                NaN                NaN                NaN                NaN   \n","\n","   feature_47_boxcox  feature_54_boxcox  feature_66_boxcox  feature_78_boxcox  \\\n","0                NaN                NaN                NaN                NaN   \n","1                NaN                NaN                NaN                NaN   \n","2                NaN                NaN                NaN                NaN   \n","3          -1.031675          -5.306281          -2.761255          -1.590256   \n","4                NaN                NaN                NaN                NaN   \n","5          -0.930557          -3.935685          -2.752938          -1.623008   \n","6                NaN                NaN                NaN                NaN   \n","7                NaN                NaN                NaN                NaN   \n","8                NaN                NaN                NaN                NaN   \n","9                NaN                NaN                NaN                NaN   \n","\n","   feature_80_boxcox  feature_88_boxcox  feature_90_boxcox  \n","0                NaN                NaN                NaN  \n","1                NaN                NaN                NaN  \n","2                NaN                NaN                NaN  \n","3          -1.645198          -0.004695          -0.641349  \n","4                NaN                NaN                NaN  \n","5          -2.022604          -0.006468          -0.641357  \n","6                NaN                NaN                NaN  \n","7                NaN                NaN                NaN  \n","8                NaN                NaN                NaN  \n","9                NaN                NaN                NaN  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["train.head(10)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Feature engineering\n","def feature_engineering(df):     \n","    # Combine all grip strength\n","    df['FGC-FGC_GS'] = df['FGC-FGC_GSD_Zone'] + df['FGC-FGC_GSND_Zone']\n","    \n","    # Combine all sit and reach\n","    df['FGC-FGC_SR'] = df['FGC-FGC_SRL_Zone'] + df['FGC-FGC_SRR_Zone']\n","    \n","    # Create a fitness score by adding the zone fitness data\n","    df['fitness_score'] = df['FGC-FGC_GS'] + df['FGC-FGC_SR'] + df['FGC-FGC_CU_Zone'] + df['FGC-FGC_PU_Zone'] + df['FGC-FGC_TL_Zone']\n","    \n","    # Combine PAQ_A-PAQ_A_Total and PAQ_C-PAQ_C_Total into one column\n","    df['PAQ_Total'] = df['PAQ_A-PAQ_A_Total'].combine_first(df['PAQ_C-PAQ_C_Total'])\n","    \n","    # Combine up to stat 11 of actigraphy stats\n","    df['combined_actigraphy_stat'] = df['feature_0'] + df['feature_1'] + df['feature_2'] + df['feature_3'] + df['feature_4'] + df['feature_5'] + df['feature_6'] + df['feature_7'] + df['feature_8']+ df['feature_9'] + df['feature_10'] + df['feature_11']\n","    \n","    # Reworking of features from other notebook\n","    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n","    \n","    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n","    df['Age_Internet_Hours'] = df['PreInt_EduHx-computerinternet_hoursday'] / df['Basic_Demos-Age']\n","    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n","    df['BIA_BMI_Internet_Hours_Age'] = (df['BIA-BIA_BMI'] * df['PreInt_EduHx-computerinternet_hoursday']) / df['Basic_Demos-Age']\n","    \n","    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n","    df['FFMI_BFP'] = df['BIA-BIA_FFMI_boxcox'] / df['BIA-BIA_Fat']\n","    df['FMI_BFP'] = df['BIA-BIA_FMI_boxcox'] / df['BIA-BIA_Fat']\n","    df['LST_TBW'] = df['BIA-BIA_LST_boxcox'] / df['BIA-BIA_TBW_boxcox']\n","    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR_boxcox']\n","    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE_boxcox']\n","    \n","    df['BMR_Weight'] = df['BIA-BIA_BMR_boxcox'] / df['Physical-Weight']\n","    df['DEE_Weight'] = df['BIA-BIA_DEE_boxcox'] / df['Physical-Weight']\n","    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n","    \n","    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI_boxcox']\n","    df['Hydration_Status'] = df['BIA-BIA_TBW_boxcox'] / df['Physical-Weight']\n","    df['ICW_TBW'] = df['BIA-BIA_ICW_boxcox'] / df['BIA-BIA_TBW_boxcox']\n","    df['Age_Weight'] = df['Basic_Demos-Age'] * df['Physical-Weight']\n","    df['Sex_BMI'] = df['Basic_Demos-Sex'] * df['Physical-BMI']\n","    df['Sex_HeartRate'] = df['Basic_Demos-Sex'] * df['Physical-HeartRate']\n","    df['Age_WaistCirc'] = df['Basic_Demos-Age'] * df['Physical-Waist_Circumference']\n","    df['BMI_FitnessMaxStage'] = df['Physical-BMI'] * df['Fitness_Endurance-Max_Stage']\n","    df['Weight_GripStrengthDominant'] = df['Physical-Weight'] * df['FGC-FGC_GSD']\n","    df['Weight_GripStrengthNonDominant'] = df['Physical-Weight'] * df['FGC-FGC_GSND']\n","    df['HeartRate_FitnessTime'] = df['Physical-HeartRate'] * (df['Fitness_Endurance-Time_Mins'] + df['Fitness_Endurance-Time_Sec'])\n","    df['Age_PushUp'] = df['Basic_Demos-Age'] * df['FGC-FGC_PU']\n","    df['FFMI_Age'] = df['BIA-BIA_FFMI_boxcox'] * df['Basic_Demos-Age']\n","    df['InternetUse_SleepDisturbance'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['SDS-SDS_Total_Raw']\n","    df['CGAS_BMI'] = df['CGAS-CGAS_Score'] * df['Physical-BMI']\n","    df['CGAS_FitnessMaxStage'] = df['CGAS-CGAS_Score'] * df['Fitness_Endurance-Max_Stage']\n","    \n","    return df"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n","          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n","          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n","\n","def update(df):\n","    global cat_c\n","    for c in cat_c: \n","        df[c] = df[c].fillna('Missing')\n","        df[c] = df[c].astype('category')\n","    return df\n","        \n","train = update(train)\n","test = update(test)\n","\n","def create_mapping(column, dataset):\n","    unique_values = dataset[column].unique()\n","    return {value: idx for idx, value in enumerate(unique_values)}\n","\n","for col in cat_c:\n","    mapping = create_mapping(col, train)\n","    mappingTe = create_mapping(col, test)\n","    \n","    train[col] = train[col].replace(mapping).astype(int)\n","    test[col] = test[col].replace(mappingTe).astype(int)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-20T04:14:23.835387Z","iopub.status.busy":"2024-10-20T04:14:23.83497Z","iopub.status.idle":"2024-10-20T04:14:31.767899Z","shell.execute_reply":"2024-10-20T04:14:31.766668Z","shell.execute_reply.started":"2024-10-20T04:14:23.835335Z"},"trusted":true},"outputs":[],"source":["imputer = KNNImputer(n_neighbors=5)\n","numeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\n","imputed_data = imputer.fit_transform(train[numeric_cols])\n","train_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n","train_imputed['sii'] = train_imputed['sii'].round().astype(int)\n","for col in train.columns:\n","    if col not in numeric_cols:\n","        train_imputed[col] = train[col]\n","        \n","train = train_imputed\n","\n","train = feature_engineering(train)\n","train = train.dropna(thresh=10, axis=0)\n","test = feature_engineering(test)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-20T04:14:31.770633Z","iopub.status.busy":"2024-10-20T04:14:31.769579Z","iopub.status.idle":"2024-10-20T04:14:32.006577Z","shell.execute_reply":"2024-10-20T04:14:32.005349Z","shell.execute_reply.started":"2024-10-20T04:14:31.770556Z"},"trusted":true},"outputs":[],"source":["train.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Export train_data to CSV\n","train_output_path = os.path.join(output_folder, 'train_data_imputed.csv')\n","train.to_csv(train_output_path, index=False)\n","print(f\"Imputed train data exported to: {train_output_path}\")\n","\n","# Export test_data to CSV\n","test_output_path = os.path.join(output_folder, 'test_data_imputed.csv')\n","test.to_csv(test_output_path, index=False)\n","print(f\"Imputed test data exported to: {test_output_path}\")\n","\n","# Make copies of data for other submissions\n","train2 = train.copy()\n","test2 = test.copy()\n","train3 = train.copy()\n","test3 = test.copy()\n","\n","print(\"Data export completed.\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-20T04:14:32.29152Z","iopub.status.busy":"2024-10-20T04:14:32.291053Z","iopub.status.idle":"2024-10-20T04:14:32.314512Z","shell.execute_reply":"2024-10-20T04:14:32.313358Z","shell.execute_reply.started":"2024-10-20T04:14:32.291467Z"},"trusted":true},"outputs":[],"source":["# Feature selection\n","# Removed stats: 'feature_92','combined_actigraphy_stat', \n","time_series_cols = [\n","    'feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4','feature_5', 'feature_6', \n","    'feature_7', 'feature_8', 'feature_9','feature_10', 'feature_11',\n","    'feature_12', 'feature_13', 'feature_14',\n","    'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19',\n","    'feature_20', 'feature_21', 'feature_22', 'feature_24',\n","    'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29',\n","    'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34',\n","    'feature_36', 'feature_37', 'feature_41', 'feature_42', 'feature_39',\n","    'feature_43', 'feature_44',\n","    'feature_45', 'feature_46', 'feature_48', 'feature_49',\n","    'feature_50', 'feature_51', 'feature_52', 'feature_53', \n","    'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59',\n","    'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64',\n","    'feature_65', 'feature_67', 'feature_68', 'feature_69',\n","    'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74',\n","    'feature_75', 'feature_76', 'feature_77', 'feature_79',\n","    'feature_81', 'feature_82', 'feature_83', 'feature_84',\n","    'feature_85', 'feature_86', 'feature_87', 'feature_89',\n","    'feature_91', 'feature_93', 'feature_94',\n","    'feature_95', 'feature_23_boxcox', 'feature_35_boxcox', 'feature_38_boxcox',\n","    'feature_40_boxcox', 'feature_47_boxcox', 'feature_54_boxcox',\n","    'feature_66_boxcox', 'feature_78_boxcox', 'feature_80_boxcox',\n","    'feature_88_boxcox', 'feature_90_boxcox'\n","]\n","\n","# Dropped columns 'FGC-FGC_CU',  'FGC-FGC_SRR', 'FGC-FGC_SRR_Zone',\n","featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex', 'FGC-FGC_GS', 'FGC-FGC_SR', \n","                'CGAS-CGAS_Score', 'Physical-BMI', 'fitness_score', 'FGC-FGC_PU',\n","                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n","                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n","                'Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins', 'FGC-FGC_CU_Zone', \n","                'FGC-FGC_GSND', 'FGC-FGC_GSD', 'FGC-FGC_PU_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n","                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC_boxcox', 'BIA-BIA_BMI',\n","                'BIA-BIA_BMR_boxcox', 'BIA-BIA_DEE_boxcox', 'BIA-BIA_ECW_boxcox', 'BIA-BIA_FFM_boxcox',\n","                'BIA-BIA_FFMI_boxcox', 'BIA-BIA_FMI_boxcox', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n","                'BIA-BIA_ICW_boxcox', 'BIA-BIA_LDM_boxcox', 'BIA-BIA_LST_boxcox', 'BIA-BIA_SMM',\n","                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total', 'PAQ_Total', 'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n","                'SDS-SDS_Total_T', 'PreInt_EduHx-computerinternet_hoursday', 'sii', 'BMI_Age',\n","                'Internet_Hours_Age','BMI_Internet_Hours', 'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n","                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW','Age_Weight','Sex_BMI','Sex_HeartRate','Age_WaistCirc',\n","                'BMI_FitnessMaxStage','Weight_GripStrengthDominant','Weight_GripStrengthNonDominant','HeartRate_FitnessTime',\n","                'Age_PushUp','FFMI_Age','InternetUse_SleepDisturbance','CGAS_BMI','CGAS_FitnessMaxStage',\n","                'Age_Internet_Hours', 'BIA_BMI_Internet_Hours_Age'\n","               ]\n","\n","featuresCols += time_series_cols\n","\n","train = train[featuresCols]\n","train = train.dropna(subset='sii')\n","\n","# Dropped columns 'FGC-FGC_CU', 'FGC-FGC_SRR', 'FGC-FGC_SRR_Zone', 'Fitness_Endurance-Time_Sec',\n","featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex', 'FGC-FGC_GS', 'FGC-FGC_SR',\n","                'CGAS-CGAS_Score', 'Physical-BMI', 'fitness_score', 'FGC-FGC_PU',\n","                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n","                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n","                'Fitness_Endurance-Max_Stage','Fitness_Endurance-Time_Mins', \n","                'FGC-FGC_CU_Zone', 'FGC-FGC_GSND', 'FGC-FGC_GSD', 'FGC-FGC_PU_Zone', \n","                'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC_boxcox', \n","                'BIA-BIA_BMI', 'BIA-BIA_BMR_boxcox', 'BIA-BIA_DEE_boxcox', 'BIA-BIA_ECW_boxcox', 'BIA-BIA_FFM_boxcox',\n","                'BIA-BIA_FFMI_boxcox', 'BIA-BIA_FMI_boxcox', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n","                'BIA-BIA_ICW_boxcox', 'BIA-BIA_LDM_boxcox', 'BIA-BIA_LST_boxcox', 'BIA-BIA_SMM',\n","                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total', 'PAQ_Total', 'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n","                'SDS-SDS_Total_T', 'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n","                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n","                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW','Age_Weight','Sex_BMI','Sex_HeartRate','Age_WaistCirc',\n","                'BMI_FitnessMaxStage','Weight_GripStrengthDominant','Weight_GripStrengthNonDominant','HeartRate_FitnessTime',\n","                'Age_PushUp','FFMI_Age','InternetUse_SleepDisturbance','CGAS_BMI','CGAS_FitnessMaxStage',\n","                'Age_Internet_Hours', 'BIA_BMI_Internet_Hours_Age']\n","\n","featuresCols += time_series_cols\n","test = test[featuresCols]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-20T04:14:32.319779Z","iopub.status.busy":"2024-10-20T04:14:32.319364Z","iopub.status.idle":"2024-10-20T04:14:32.517875Z","shell.execute_reply":"2024-10-20T04:14:32.516592Z","shell.execute_reply.started":"2024-10-20T04:14:32.319736Z"},"trusted":true},"outputs":[],"source":["train.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-20T04:14:32.519826Z","iopub.status.busy":"2024-10-20T04:14:32.519363Z","iopub.status.idle":"2024-10-20T04:14:32.798569Z","shell.execute_reply":"2024-10-20T04:14:32.79738Z","shell.execute_reply.started":"2024-10-20T04:14:32.519776Z"},"trusted":true},"outputs":[],"source":["test.head(10)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-10-20T04:14:32.800698Z","iopub.status.busy":"2024-10-20T04:14:32.800157Z","iopub.status.idle":"2024-10-20T04:14:32.815922Z","shell.execute_reply":"2024-10-20T04:14:32.814675Z","shell.execute_reply.started":"2024-10-20T04:14:32.80064Z"},"trusted":true},"outputs":[],"source":["# Replace inf\n","if np.any(np.isinf(train)):\n","    train = train.replace([np.inf, -np.inf], np.nan)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Model parameters for tuning\n","def quadratic_weighted_kappa(y_true, y_pred):\n","    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n","\n","def threshold_Rounder(oof_non_rounded, thresholds):\n","    return np.where(oof_non_rounded < thresholds[0], 0,\n","                    np.where(oof_non_rounded < thresholds[1], 1,\n","                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n","\n","def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n","    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n","    return -quadratic_weighted_kappa(y_true, rounded_p)\n","\n","# Hyperparameter tuning\n","def tune_hyperparameters(X, y):\n","    models = [LGBMClassifier, XGBClassifier, CatBoostClassifier]\n","    best_params = {}\n","\n","    for model_class in models:\n","        study = optuna.create_study(direction='maximize')\n","        study.optimize(lambda trial: objective(trial, X, y, model_class), n_trials=400)\n","        best_params[model_class.__name__] = study.best_params\n","\n","    return best_params\n","\n","def objective(trial, X, y, model_class):\n","    if model_class == LGBMClassifier:\n","        params = {\n","            'n_estimators': trial.suggest_int('n_estimators', 650, 900),\n","            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.05, log=True),\n","            'num_leaves': trial.suggest_int('num_leaves', 700, 1200),\n","            'max_depth': trial.suggest_int('max_depth', 3, 15),\n","            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n","            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n","            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n","            'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n","            'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10, log=True),\n","            'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10, log=True),\n","        }\n","    elif model_class == XGBClassifier:\n","        params = {\n","            'max_depth': trial.suggest_int('max_depth', 7, 15),\n","            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n","            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n","            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n","            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n","            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n","            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n","            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n","            'tree_method': 'exact'\n","        }\n","    elif model_class == CatBoostClassifier:\n","        params = {\n","            'iterations': trial.suggest_int('iterations', 300, 600),\n","            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n","            'depth': trial.suggest_int('depth', 5, 10),\n","            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10, log=True),\n","            'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n","            'random_strength': trial.suggest_float('random_strength', 1e-9, 1),\n","        }\n","        if params['bootstrap_type'] == 'Bayesian':\n","            params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 10)\n","        elif params['bootstrap_type'] == 'Bernoulli':\n","            params['subsample'] = trial.suggest_float('subsample', 0.1, 1)\n","    \n","    model = model_class(**params, random_state=42)\n","    score = cross_val_score(model, X, y, cv=5, scoring=make_scorer(quadratic_weighted_kappa))\n","    return score.mean()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Hypertuning model\n","def TrainML(X, y, X_test, n_splits=5, SEED=42):\n","    X = train.drop(['sii'], axis=1)\n","    y = train['sii']\n","\n","    # Tune hyperparameters\n","    print(\"Tuning hyperparameters...\")\n","    best_params = tune_hyperparameters(X, y)\n","\n","    # Create models with tuned hyperparameters\n","    lgbm_model = LGBMClassifier(**best_params['LGBMClassifier'], random_state=SEED)\n","    xgb_model = XGBClassifier(**best_params['XGBClassifier'], random_state=SEED)\n","    catboost_model = CatBoostClassifier(**best_params['CatBoostClassifier'], random_state=SEED)\n","\n","    # Create VotingClassifier\n","    voting_model = VotingClassifier([\n","        ('lgbm', lgbm_model),\n","        ('xgb', xgb_model),\n","        ('catboost', catboost_model)\n","    ], voting='soft')\n","    \n","    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n","    \n","    oof_preds = np.zeros((len(X), 4))  # 4 classes\n","    test_preds = np.zeros((len(X_test), 4))\n","\n","    for fold, (train_idx, val_idx) in enumerate(tqdm(SKF.split(X, y), total=n_splits, desc=\"Training folds\")):\n","        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n","        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n","\n","        voting_model.fit(X_train, y_train)\n","\n","        oof_preds[val_idx] = voting_model.predict_proba(X_val)\n","        test_preds += voting_model.predict_proba(X_test) / n_splits\n","\n","        val_preds = np.argmax(oof_preds[val_idx], axis=1)\n","        val_score = quadratic_weighted_kappa(y_val, val_preds)\n","        print(f\"Fold {fold + 1} Validation QWK: {val_score:.4f}\")\n","\n","    oof_preds_class = np.argmax(oof_preds, axis=1)\n","    overall_score = quadratic_weighted_kappa(y, oof_preds_class)\n","    print(f\"Overall QWK Score: {overall_score:.4f}\")\n","\n","    submission = pd.DataFrame({\n","        'id': sample_submission['id'],\n","        'sii': np.argmax(test_preds, axis=1)\n","    })\n","\n","    with open(os.path.join(output_folder, 'best_params.json'), 'w') as f:\n","        json.dump(best_params, f)\n","\n","    return submission, overall_score, best_params"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Submission_hyper_tuned = TrainML( test)\n","\n","# Save submission\n","Submission_hyper_tuned.to_csv('submission_1.csv', index=False)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-10-20T04:14:32.817914Z","iopub.status.busy":"2024-10-20T04:14:32.817522Z","iopub.status.idle":"2024-10-20T04:14:32.836683Z","shell.execute_reply":"2024-10-20T04:14:32.8354Z","shell.execute_reply.started":"2024-10-20T04:14:32.817875Z"},"trusted":true},"outputs":[],"source":["SEED = 42\n","n_splits = 5\n","\n","def TrainML(model_class, test_data):\n","    X = train.drop(['sii'], axis=1)\n","    y = train['sii']\n","\n","    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n","    \n","    train_S = []\n","    test_S = []\n","    \n","    oof_non_rounded = np.zeros(len(y), dtype=float) \n","    oof_rounded = np.zeros(len(y), dtype=int) \n","    test_preds = np.zeros((len(test_data), n_splits))\n","\n","    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n","        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n","        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n","\n","        model = clone(model_class)\n","        model.fit(X_train, y_train)\n","\n","        y_train_pred = model.predict(X_train)\n","        y_val_pred = model.predict(X_val)\n","\n","        oof_non_rounded[test_idx] = y_val_pred\n","        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n","        oof_rounded[test_idx] = y_val_pred_rounded\n","\n","        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n","        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n","\n","        train_S.append(train_kappa)\n","        test_S.append(val_kappa)\n","        \n","        test_preds[:, fold] = model.predict(test_data)\n","        \n","        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n","        clear_output(wait=True)\n","\n","    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n","    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n","\n","    KappaOPtimizer = minimize(evaluate_predictions,\n","                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n","                              method='Nelder-Mead')\n","    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n","    \n","    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n","    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n","\n","    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n","\n","    tpm = test_preds.mean(axis=1)\n","    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n","    \n","    submission = pd.DataFrame({\n","        'id': sample['id'],\n","        'sii': tpTuned\n","    })\n","\n","    return submission"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-10-20T04:14:32.83891Z","iopub.status.busy":"2024-10-20T04:14:32.838438Z","iopub.status.idle":"2024-10-20T04:14:32.857018Z","shell.execute_reply":"2024-10-20T04:14:32.855698Z","shell.execute_reply.started":"2024-10-20T04:14:32.838859Z"},"trusted":true},"outputs":[],"source":["# Model parameters from hyper-tuning\n","Params = {\n","    'n_estimators': 656,\n","    'learning_rate': 0.046,\n","    'max_depth': 12,\n","    'num_leaves': 478,\n","    'min_data_in_leaf': 13,\n","    'feature_fraction': 0.893,\n","    'bagging_fraction': 0.784,\n","    'bagging_freq': 4,\n","    'lambda_l1': 10,  # Increased from 6.59\n","    'lambda_l2': 0.01  # Increased from 2.68e-06\n","}\n","\n","\n","XGB_Params = {\n","    'learning_rate': 0.05,\n","    'max_depth': 6,\n","    'n_estimators': 200,\n","    'subsample': 0.8,\n","    'colsample_bytree': 0.8,\n","    'reg_alpha': 1,  # Increased from 0.1\n","    'reg_lambda': 5,  # Increased from 1\n","    'random_state': SEED,\n","    'tree_method': 'exact'\n","}\n","\n","\n","CatBoost_Params = {\n","    'learning_rate': 0.05,\n","    'depth': 6,\n","    'iterations': 200,\n","    'random_seed': SEED,\n","    'verbose': 0,\n","    'l2_leaf_reg': 10  # Increase this value\n","}\n","\n","# Create model instances\n","Light = LGBMRegressor(**Params, random_state=SEED, verbose=-1)\n","XGB_Model = XGBRegressor(**XGB_Params)\n","CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n","\n","# Combine models using Voting Regressor\n","voting_model = VotingRegressor(estimators=[\n","    ('lightgbm', Light),\n","    ('xgboost', XGB_Model),\n","    ('catboost', CatBoost_Model)\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-20T04:14:32.858873Z","iopub.status.busy":"2024-10-20T04:14:32.858492Z","iopub.status.idle":"2024-10-20T04:15:42.667115Z","shell.execute_reply":"2024-10-20T04:15:42.665844Z","shell.execute_reply.started":"2024-10-20T04:14:32.858835Z"},"trusted":true},"outputs":[],"source":["Submission1 = TrainML(voting_model, test)\n","\n","# Save submission\n","Submission1.to_csv('submission_1.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-20T04:15:42.669354Z","iopub.status.busy":"2024-10-20T04:15:42.668834Z","iopub.status.idle":"2024-10-20T04:15:42.682268Z","shell.execute_reply":"2024-10-20T04:15:42.681123Z","shell.execute_reply.started":"2024-10-20T04:15:42.669258Z"},"trusted":true},"outputs":[],"source":["Submission1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-20T04:15:42.684744Z","iopub.status.busy":"2024-10-20T04:15:42.684226Z","iopub.status.idle":"2024-10-20T04:18:25.16235Z","shell.execute_reply":"2024-10-20T04:18:25.161144Z","shell.execute_reply.started":"2024-10-20T04:15:42.684692Z"},"trusted":true},"outputs":[],"source":["# Submission #2\n","# Removed  \n","time_series_cols = [\n","    'feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4','feature_5', 'feature_6', \n","    'feature_7', 'feature_8', 'feature_9','feature_10', 'feature_11',\n","    'feature_12', 'feature_13', 'feature_14',\n","    'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19',\n","    'feature_20', 'feature_21', 'feature_22', 'feature_24',\n","    'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29',\n","    'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34',\n","    'feature_36', 'feature_37', 'feature_41', 'feature_42', 'feature_39',\n","    'feature_43', 'feature_44',\n","    'feature_45', 'feature_46', 'feature_48', 'feature_49',\n","    'feature_50', 'feature_51', 'feature_52', 'feature_53', \n","    'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59',\n","    'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64',\n","    'feature_65', 'feature_67', 'feature_68', 'feature_69',\n","    'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74',\n","    'feature_75', 'feature_76', 'feature_77', 'feature_79',\n","    'feature_81', 'feature_82', 'feature_83', 'feature_84',\n","    'feature_85', 'feature_86', 'feature_87', 'feature_89',\n","    'feature_91', 'feature_93', 'feature_94',\n","    'feature_95', 'feature_23_boxcox', 'feature_35_boxcox', 'feature_38_boxcox',\n","    'feature_40_boxcox', 'feature_47_boxcox', 'feature_54_boxcox',\n","    'feature_66_boxcox', 'feature_78_boxcox', 'feature_80_boxcox',\n","    'feature_88_boxcox', 'feature_90_boxcox'\n","]\n","\n","featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n","                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n","                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n","                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n","                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n","                'Fitness_Endurance-Time_Mins', \n","                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n","                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n","                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n","                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n","                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC_boxcox', 'BIA-BIA_BMI',\n","                'BIA-BIA_BMR_boxcox', 'BIA-BIA_DEE_boxcox', 'BIA-BIA_ECW_boxcox', 'BIA-BIA_FFM_boxcox',\n","                'BIA-BIA_FFMI_boxcox', 'BIA-BIA_FMI_boxcox', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n","                'BIA-BIA_ICW_boxcox', 'BIA-BIA_LDM_boxcox', 'BIA-BIA_LST_boxcox', 'BIA-BIA_SMM',\n","                'BIA-BIA_TBW_boxcox', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n","                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n","                'SDS-SDS_Total_T', 'PreInt_EduHx-Season', 'fitness_score',\n","                'PreInt_EduHx-computerinternet_hoursday', 'sii',\n","                'Age_Internet_Hours', 'BIA_BMI_Internet_Hours_Age']\n","\n","featuresCols += time_series_cols\n","\n","train2 = train2[featuresCols]\n","train2 = train2.dropna(subset='sii')\n","\n","featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n","                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n","                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n","                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n","                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n","                'Fitness_Endurance-Time_Mins', \n","                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n","                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n","                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n","                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n","                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC_boxcox', 'BIA-BIA_BMI',\n","                'BIA-BIA_BMR_boxcox', 'BIA-BIA_DEE_boxcox', 'BIA-BIA_ECW_boxcox', 'BIA-BIA_FFM_boxcox',\n","                'BIA-BIA_FFMI_boxcox', 'BIA-BIA_FMI_boxcox', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n","                'BIA-BIA_ICW_boxcox', 'BIA-BIA_LDM_boxcox', 'BIA-BIA_LST_boxcox', 'BIA-BIA_SMM',\n","                'BIA-BIA_TBW_boxcox', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n","                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n","                'SDS-SDS_Total_T', 'PreInt_EduHx-Season', 'fitness_score',\n","                'PreInt_EduHx-computerinternet_hoursday',\n","                'Age_Internet_Hours', 'BIA_BMI_Internet_Hours_Age']\n","\n","featuresCols += time_series_cols\n","test2 = test2[featuresCols]\n","\n","cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n","          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n","          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n","\n","def update(df):\n","    global cat_c\n","    for c in cat_c: \n","        df[c] = df[c].fillna('Missing')\n","        df[c] = df[c].astype('category')\n","    return df\n","        \n","train2 = update(train2)\n","test2 = update(test2)\n","\n","def create_mapping(column, dataset):\n","    unique_values = dataset[column].unique()\n","    return {value: idx for idx, value in enumerate(unique_values)}\n","\n","for col in cat_c:\n","    mapping = create_mapping(col, train2)\n","    mappingTe = create_mapping(col, test2)\n","    \n","    train2[col] = train2[col].replace(mapping).astype(int)\n","    test2[col] = test2[col].replace(mappingTe).astype(int)\n","\n","def quadratic_weighted_kappa(y_true, y_pred):\n","    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n","\n","def threshold_Rounder(oof_non_rounded, thresholds):\n","    return np.where(oof_non_rounded < thresholds[0], 0,\n","                    np.where(oof_non_rounded < thresholds[1], 1,\n","                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n","\n","def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n","    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n","    return -quadratic_weighted_kappa(y_true, rounded_p)\n","\n","def TrainML(model_class, test_data):\n","    X = train2.drop(['sii'], axis=1)\n","    y = train2['sii']\n","\n","    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n","    \n","    train2_S = []\n","    test2_S = []\n","    \n","    oof_non_rounded = np.zeros(len(y), dtype=float) \n","    oof_rounded = np.zeros(len(y), dtype=int) \n","    test_preds = np.zeros((len(test_data), n_splits))\n","\n","    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n","        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n","        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n","\n","        model = clone(model_class)\n","        model.fit(X_train, y_train)\n","\n","        y_train_pred = model.predict(X_train)\n","        y_val_pred = model.predict(X_val)\n","\n","        oof_non_rounded[test_idx] = y_val_pred\n","        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n","        oof_rounded[test_idx] = y_val_pred_rounded\n","\n","        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n","        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n","\n","        train2_S.append(train_kappa)\n","        test2_S.append(val_kappa)\n","        \n","        test_preds[:, fold] = model.predict(test_data)\n","        \n","        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n","        clear_output(wait=True)\n","\n","    print(f\"Mean Train QWK --> {np.mean(train2_S):.4f}\")\n","    print(f\"Mean Validation QWK ---> {np.mean(test2_S):.4f}\")\n","\n","    KappaOPtimizer = minimize(evaluate_predictions,\n","                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n","                              method='Nelder-Mead')\n","    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n","    \n","    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n","    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n","\n","    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n","\n","    tpm = test_preds.mean(axis=1)\n","    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n","    \n","    submission = pd.DataFrame({\n","        'id': sample['id'],\n","        'sii': tpTuned\n","    })\n","\n","    return submission\n","\n","# Model parameters for LightGBM\n","Params = {\n","    'n_estimators': 682,\n","    'learning_rate': 0.022201704131134002,\n","    'max_depth': 3,\n","    'num_leaves': 843,\n","    'min_data_in_leaf': 13,\n","    'feature_fraction': 0.893,\n","    'bagging_fraction': 0.8918812900108436,\n","    'bagging_freq': 10,\n","    'lambda_l1': 4.79779460021304e-07,\n","    'lambda_l2': 2.0055171376757653e-06,\n","    'min_child_samples': 39,\n","    'colsample_bytree': 0.9264391369678474\n","}\n","\n","\n","# XGBoost parameters\n","XGB_Params = {\n","    'learning_rate': 0.02829495436971426,\n","    'max_depth': 8,\n","    'n_estimators': 484,\n","    'subsample': 0.9834706801888403,\n","    'colsample_bytree': 0.7681852816292032,\n","    'reg_alpha': 0.010495697417466835,\n","    'reg_lambda': 0.022138771647168275,\n","    'random_state': SEED,\n","    'tree_method': 'exact',\n","    'min_child_weight': 9\n","}\n","\n","\n","CatBoost_Params = {\n","    'learning_rate': 0.09981560408272906,\n","    'depth': 6,\n","    'iterations': 43,\n","    'random_seed': SEED,\n","    'verbose': 0,\n","    'l2_leaf_reg': 3.4155853181132496,\n","    'bootstrap_type': 'MVS',\n","    'random_strength': 0.06930624569348895\n","}\n","\n","# Create model instances\n","Light = LGBMRegressor(**Params, random_state=SEED, verbose=-1)\n","XGB_Model = XGBRegressor(**XGB_Params)\n","CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n","\n","# Combine models using Voting Regressor\n","voting_model = VotingRegressor(estimators=[\n","    ('lightgbm', Light),\n","    ('xgboost', XGB_Model),\n","    ('catboost', CatBoost_Model)\n","])\n","\n","# Train the ensemble model\n","Submission2 = TrainML(voting_model, test2)\n","\n","# Save submission\n","Submission2.to_csv('submission_2.csv', index=False)\n","Submission2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-20T04:18:25.18058Z","iopub.status.busy":"2024-10-20T04:18:25.180146Z","iopub.status.idle":"2024-10-20T04:21:24.26648Z","shell.execute_reply":"2024-10-20T04:21:24.265352Z","shell.execute_reply.started":"2024-10-20T04:18:25.180525Z"},"trusted":true},"outputs":[],"source":["# Submission 3\n","time_series_cols = [\n","    'feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4','feature_5', 'feature_6', \n","    'feature_7', 'feature_8', 'feature_9','feature_10', 'feature_11',\n","    'feature_12', 'feature_13', 'feature_14',\n","    'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19',\n","    'feature_20', 'feature_21', 'feature_22', 'feature_24',\n","    'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29',\n","    'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34',\n","    'feature_36', 'feature_37', 'feature_41', 'feature_42', 'feature_39',\n","    'feature_43', 'feature_44',\n","    'feature_45', 'feature_46', 'feature_48', 'feature_49',\n","    'feature_50', 'feature_51', 'feature_52', 'feature_53', \n","    'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59',\n","    'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64',\n","    'feature_65', 'feature_67', 'feature_68', 'feature_69',\n","    'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74',\n","    'feature_75', 'feature_76', 'feature_77', 'feature_79',\n","    'feature_81', 'feature_82', 'feature_83', 'feature_84',\n","    'feature_85', 'feature_86', 'feature_87', 'feature_89',\n","    'feature_91', 'feature_93', 'feature_94',\n","    'feature_95', 'feature_23_boxcox', 'feature_35_boxcox', 'feature_38_boxcox',\n","    'feature_40_boxcox', 'feature_47_boxcox', 'feature_54_boxcox',\n","    'feature_66_boxcox', 'feature_78_boxcox', 'feature_80_boxcox',\n","    'feature_88_boxcox', 'feature_90_boxcox'\n","]\n","\n","featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n","                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n","                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n","                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n","                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n","                'Fitness_Endurance-Time_Mins', \n","                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n","                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n","                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n","                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n","                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n","                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n","                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n","                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n","                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n","                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n","                'SDS-SDS_Total_T', 'PreInt_EduHx-Season', 'fitness_score',\n","                'PreInt_EduHx-computerinternet_hoursday', 'sii',\n","                'Age_Internet_Hours', 'BIA_BMI_Internet_Hours_Age']\n","\n","featuresCols += time_series_cols\n","\n","train3 = train3[featuresCols]\n","train3 = train3.dropna(subset='sii')\n","\n","featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n","                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n","                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n","                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n","                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n","                'Fitness_Endurance-Time_Mins', \n","                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n","                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n","                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n","                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n","                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n","                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n","                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n","                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n","                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n","                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n","                'SDS-SDS_Total_T', 'PreInt_EduHx-Season', 'fitness_score',\n","                'PreInt_EduHx-computerinternet_hoursday', \n","                'Age_Internet_Hours', 'BIA_BMI_Internet_Hours_Age']\n","\n","featuresCols += time_series_cols\n","test3 = test3[featuresCols]\n","\n","def quadratic_weighted_kappa(y_true, y_pred):\n","    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n","\n","def threshold_Rounder(oof_non_rounded, thresholds):\n","    return np.where(oof_non_rounded < thresholds[0], 0,\n","                    np.where(oof_non_rounded < thresholds[1], 1,\n","                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n","\n","def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n","    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n","    return -quadratic_weighted_kappa(y_true, rounded_p)\n","\n","def TrainML(model_class, test_data):\n","    X = train3.drop(['sii'], axis=1)\n","    y = train3['sii']\n","\n","    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n","    \n","    train_S = []\n","    test_S = []\n","    \n","    oof_non_rounded = np.zeros(len(y), dtype=float) \n","    oof_rounded = np.zeros(len(y), dtype=int) \n","    test_preds = np.zeros((len(test_data), n_splits))\n","\n","    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n","        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n","        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n","\n","        model = clone(model_class)\n","        model.fit(X_train, y_train)\n","\n","        y_train_pred = model.predict(X_train)\n","        y_val_pred = model.predict(X_val)\n","\n","        oof_non_rounded[test_idx] = y_val_pred\n","        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n","        oof_rounded[test_idx] = y_val_pred_rounded\n","\n","        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n","        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n","\n","        train_S.append(train_kappa)\n","        test_S.append(val_kappa)\n","        \n","        test_preds[:, fold] = model.predict(test_data)\n","        \n","        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n","        clear_output(wait=True)\n","\n","    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n","    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n","\n","    KappaOPtimizer = minimize(evaluate_predictions,\n","                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n","                              method='Nelder-Mead')\n","    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n","    \n","    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n","    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n","\n","    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n","\n","    tpm = test_preds.mean(axis=1)\n","    tp_rounded = threshold_Rounder(tpm, KappaOPtimizer.x)\n","\n","    return tp_rounded\n","\n","imputer = SimpleImputer(strategy='median')\n","\n","ensemble = VotingRegressor(estimators=[\n","    ('lgb', Pipeline(steps=[('imputer', imputer), ('regressor', LGBMRegressor(random_state=SEED))])),\n","    ('xgb', Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])),\n","    ('cat', Pipeline(steps=[('imputer', imputer), ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])),\n","    ('rf', Pipeline(steps=[('imputer', imputer), ('regressor', RandomForestRegressor(random_state=SEED))])),\n","    ('gb', Pipeline(steps=[('imputer', imputer), ('regressor', GradientBoostingRegressor(random_state=SEED))]))\n","])\n","\n","Submission3 = TrainML(ensemble, test3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-20T04:21:24.26819Z","iopub.status.busy":"2024-10-20T04:21:24.26786Z","iopub.status.idle":"2024-10-20T04:21:24.281624Z","shell.execute_reply":"2024-10-20T04:21:24.280338Z","shell.execute_reply.started":"2024-10-20T04:21:24.268153Z"},"trusted":true},"outputs":[],"source":["Submission3 = pd.DataFrame({\n","    'id': sample['id'],\n","    'sii': Submission3\n","})\n","\n","Submission3"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-20T04:21:24.283679Z","iopub.status.busy":"2024-10-20T04:21:24.283253Z","iopub.status.idle":"2024-10-20T04:21:24.307013Z","shell.execute_reply":"2024-10-20T04:21:24.305885Z","shell.execute_reply.started":"2024-10-20T04:21:24.28364Z"},"trusted":true},"outputs":[],"source":["sub1 = Submission1\n","sub2 = Submission2\n","sub3 = Submission3\n","\n","sub1 = sub1.sort_values(by='id').reset_index(drop=True)\n","sub2 = sub2.sort_values(by='id').reset_index(drop=True)\n","sub3 = sub3.sort_values(by='id').reset_index(drop=True)\n","\n","combined = pd.DataFrame({\n","    'id': sub1['id'],\n","    'sii_1': sub1['sii'],\n","    'sii_2': sub2['sii'],\n","    'sii_3': sub3['sii']\n","})\n","\n","def majority_vote(row):\n","    return row.mode()[0]\n","\n","combined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3']].apply(majority_vote, axis=1)\n","\n","final_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n","\n","final_submission.to_csv('submission.csv', index=False)\n","\n","print(\"Majority voting completed and saved to 'Final_Submission.csv'\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-20T04:21:24.309578Z","iopub.status.busy":"2024-10-20T04:21:24.308765Z","iopub.status.idle":"2024-10-20T04:21:24.325904Z","shell.execute_reply":"2024-10-20T04:21:24.32484Z","shell.execute_reply.started":"2024-10-20T04:21:24.309533Z"},"trusted":true},"outputs":[],"source":["final_submission"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":9643020,"sourceId":81933,"sourceType":"competition"}],"dockerImageVersionId":30786,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":4}
